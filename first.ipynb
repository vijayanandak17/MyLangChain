{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5277f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting langchain-core<2.0.0,>=0.3.75 (from langchain-community)\n",
      "  Downloading langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting langchain<2.0.0,>=0.3.27 (from langchain-community)\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain-community)\n",
      "  Downloading sqlalchemy-2.0.43-cp311-cp311-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: requests<3,>=2.32.5 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-community) (6.0.2)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
      "  Downloading aiohttp-3.12.15-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-community) (9.1.2)\n",
      "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting langsmith>=0.1.125 (from langchain-community)\n",
      "  Downloading langsmith-0.4.27-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-community) (2.2.6)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading multidict-6.6.4-cp311-cp311-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading propcache-0.3.2-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading yarl-1.20.1-cp311-cp311-win_amd64.whl.metadata (76 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain<2.0.0,>=0.3.27->langchain-community)\n",
      "  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain<2.0.0,>=0.3.27->langchain-community)\n",
      "  Downloading pydantic-2.11.9-py3-none-any.whl.metadata (68 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<2.0.0,>=0.3.75->langchain-community)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\vijay\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (25.0)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain-community)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.32.5->langchain-community) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.32.5->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.32.5->langchain-community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.32.5->langchain-community) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith>=0.1.125->langchain-community)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson>=3.9.14 (from langsmith>=0.1.125->langchain-community)\n",
      "  Downloading orjson-3.11.3-cp311-cp311-win_amd64.whl.metadata (43 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.125->langchain-community)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith>=0.1.125->langchain-community)\n",
      "  Downloading zstandard-0.25.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community)\n",
      "  Downloading anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
      "Downloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 1.0/2.5 MB 4.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.1/2.5 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 4.8 MB/s  0:00:00\n",
      "Downloading aiohttp-3.12.15-cp311-cp311-win_amd64.whl (453 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 0.8/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 2.1 MB/s  0:00:00\n",
      "Downloading langchain_core-0.3.76-py3-none-any.whl (447 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading multidict-6.6.4-cp311-cp311-win_amd64.whl (46 kB)\n",
      "Downloading pydantic-2.11.9-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/2.0 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.8/2.0 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 2.9 MB/s  0:00:00\n",
      "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "Downloading sqlalchemy-2.0.43-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.8/2.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 2.8 MB/s  0:00:00\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.20.1-cp311-cp311-win_amd64.whl (86 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading langsmith-0.4.27-py3-none-any.whl (384 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading orjson-3.11.3-cp311-cp311-win_amd64.whl (131 kB)\n",
      "Downloading propcache-0.3.2-cp311-cp311-win_amd64.whl (41 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading zstandard-0.25.0-cp311-cp311-win_amd64.whl (506 kB)\n",
      "Downloading anyio-4.10.0-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: zstandard, typing-inspection, SQLAlchemy, pydantic-core, propcache, orjson, mypy-extensions, multidict, marshmallow, jsonpointer, httpx-sse, httpcore, frozenlist, anyio, annotated-types, aiohappyeyeballs, yarl, typing-inspect, requests-toolbelt, pydantic, jsonpatch, httpx, aiosignal, pydantic-settings, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
      "\n",
      "   - --------------------------------------  1/31 [typing-inspection]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   -- -------------------------------------  2/31 [SQLAlchemy]\n",
      "   --- ------------------------------------  3/31 [pydantic-core]\n",
      "   ------ ---------------------------------  5/31 [orjson]\n",
      "   ---------- -----------------------------  8/31 [marshmallow]\n",
      "   ---------- -----------------------------  8/31 [marshmallow]\n",
      "   ---------- -----------------------------  8/31 [marshmallow]\n",
      "   ------------ --------------------------- 10/31 [httpx-sse]\n",
      "   -------------- ------------------------- 11/31 [httpcore]\n",
      "   -------------- ------------------------- 11/31 [httpcore]\n",
      "   -------------- ------------------------- 11/31 [httpcore]\n",
      "   -------------- ------------------------- 11/31 [httpcore]\n",
      "   -------------- ------------------------- 11/31 [httpcore]\n",
      "   -------------- ------------------------- 11/31 [httpcore]\n",
      "   --------------- ------------------------ 12/31 [frozenlist]\n",
      "   ---------------- ----------------------- 13/31 [anyio]\n",
      "   ---------------- ----------------------- 13/31 [anyio]\n",
      "   ---------------- ----------------------- 13/31 [anyio]\n",
      "   ---------------- ----------------------- 13/31 [anyio]\n",
      "   ---------------- ----------------------- 13/31 [anyio]\n",
      "   ---------------- ----------------------- 13/31 [anyio]\n",
      "   ------------------ --------------------- 14/31 [annotated-types]\n",
      "   ------------------- -------------------- 15/31 [aiohappyeyeballs]\n",
      "   -------------------- ------------------- 16/31 [yarl]\n",
      "   -------------------- ------------------- 16/31 [yarl]\n",
      "   ----------------------- ---------------- 18/31 [requests-toolbelt]\n",
      "   ----------------------- ---------------- 18/31 [requests-toolbelt]\n",
      "   ----------------------- ---------------- 18/31 [requests-toolbelt]\n",
      "   ----------------------- ---------------- 18/31 [requests-toolbelt]\n",
      "   ----------------------- ---------------- 18/31 [requests-toolbelt]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------- -------------- 20/31 [jsonpatch]\n",
      "   --------------------------- ------------ 21/31 [httpx]\n",
      "   --------------------------- ------------ 21/31 [httpx]\n",
      "   --------------------------- ------------ 21/31 [httpx]\n",
      "   --------------------------- ------------ 21/31 [httpx]\n",
      "   --------------------------- ------------ 21/31 [httpx]\n",
      "   --------------------------- ------------ 21/31 [httpx]\n",
      "   ---------------------------- ----------- 22/31 [aiosignal]\n",
      "   ----------------------------- ---------- 23/31 [pydantic-settings]\n",
      "   ----------------------------- ---------- 23/31 [pydantic-settings]\n",
      "   ----------------------------- ---------- 23/31 [pydantic-settings]\n",
      "   ------------------------------ --------- 24/31 [langsmith]\n",
      "   ------------------------------ --------- 24/31 [langsmith]\n",
      "   ------------------------------ --------- 24/31 [langsmith]\n",
      "   ------------------------------ --------- 24/31 [langsmith]\n",
      "   ------------------------------ --------- 24/31 [langsmith]\n",
      "   ------------------------------ --------- 24/31 [langsmith]\n",
      "   ------------------------------ --------- 24/31 [langsmith]\n",
      "   ------------------------------ --------- 24/31 [langsmith]\n",
      "   ------------------------------ --------- 24/31 [langsmith]\n",
      "   -------------------------------- ------- 25/31 [dataclasses-json]\n",
      "   -------------------------------- ------- 25/31 [dataclasses-json]\n",
      "   --------------------------------- ------ 26/31 [aiohttp]\n",
      "   --------------------------------- ------ 26/31 [aiohttp]\n",
      "   --------------------------------- ------ 26/31 [aiohttp]\n",
      "   --------------------------------- ------ 26/31 [aiohttp]\n",
      "   --------------------------------- ------ 26/31 [aiohttp]\n",
      "   --------------------------------- ------ 26/31 [aiohttp]\n",
      "   --------------------------------- ------ 26/31 [aiohttp]\n",
      "   --------------------------------- ------ 26/31 [aiohttp]\n",
      "   --------------------------------- ------ 26/31 [aiohttp]\n",
      "   --------------------------------- ------ 26/31 [aiohttp]\n",
      "   --------------------------------- ------ 26/31 [aiohttp]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ------------------------------------ --- 28/31 [langchain-text-splitters]\n",
      "   ------------------------------------ --- 28/31 [langchain-text-splitters]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   ------------------------------------- -- 29/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain-community]\n",
      "   ---------------------------------------- 31/31 [langchain-community]\n",
      "\n",
      "Successfully installed SQLAlchemy-2.0.43 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 annotated-types-0.7.0 anyio-4.10.0 dataclasses-json-0.6.7 frozenlist-1.7.0 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.27 langchain-community-0.3.29 langchain-core-0.3.76 langchain-text-splitters-0.3.11 langsmith-0.4.27 marshmallow-3.26.1 multidict-6.6.4 mypy-extensions-1.1.0 orjson-3.11.3 propcache-0.3.2 pydantic-2.11.9 pydantic-core-2.33.2 pydantic-settings-2.10.1 requests-toolbelt-1.0.0 typing-inspect-0.9.0 typing-inspection-0.4.1 yarl-1.20.1 zstandard-0.25.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7cb4770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b440c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=TextLoader(\"notes.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dfc4c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'notes.txt'}, page_content='Thsi is a good demo for Langchain')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d86fec94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-6.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Downloading pypdf-6.0.0-py3-none-any.whl (310 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-6.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86657aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd928b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = PyPDFLoader(\"resume.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc2e1407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-29T06:10:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-29T06:10:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'resume.pdf', 'total_pages': 1, 'page': 0, 'page_label': ''}, page_content='John Doe  Software Developer  Prague, Czech Republic  john.doe@gmail.com  +420 123 456 789\\nJOHN DOE RESUME\\nEnnatha: Developer at The Company, M.Sc. in Computer Science\\nSkills: Java, C#/.Net, C++, Python, JavaScript, Ruby, Bash, SQL\\nInterests: Data Warehouses, Data Lakes, Data Analysis, Data Quality\\nActivities: Hockey, Football, Tennis, Basketball, Reading, Music\\nSummary\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque gravida ligula sed rhoncus lobortis. Pellen-\\ntesque sit amet sapien in neque cursus mollis. Nulla aliquet mauris ac enim fermentum tincidunt. Donec eu felis\\nut purus ullamcorper aliquet eget at lorem. Suspendisse vulputate mi libero, sit amet sollicitudin enim suscipit\\nin. Curabitur laoreet sem ultrices, consequat neque sed, vulputate purus. Sed in.\\nExperience\\n16/01 - now Developer The Company\\nProin laoreet dolor vitae velit tristique, id interdum augue finibus quisque non ades\\nErat at purus facilisis vestibulum pulvinar sit amet felis, mauris laoreet justo nec err in\\nLorem consectetur elementum, aliquam facilisis ante id magna porta mattis, vivamus\\nMorbi sit amet ullamcorper felis fusce nec mi ac nisi cursus aliquet, vestibulum volutpat\\nVestibulum quam lectus, tempus in urna semper, finibus consequat mauris, quisque\\n15/01 - 15/12 Developer The Older Company\\nPraesent aliquam sagittis hendrerit phasellus efficitur tincidunt et (https:/ /google.com)\\nAmet eget augue nam quis sapien eget arcu placerat lobortis vivamus maximus elit id\\nEget lacus nec dolor sagittis efficitur aliquam nec metus vitae justo auctor sit erat arr\\nCondimentum vestibulum, nullam vitae cursus erat, praesent hendrerit leo a turpis\\n14/01 - 14/12 Intern The Oldest Company\\nQuisque in lacus lorem. In vitae feugiat leo mauris a hendrerit felis fusce mi mattis\\nViverra ut sem non, commodo dictum leo nullam pulvinar mattis nisi quis iaculis ex\\nNulla finibus leo lectus, sed feugiat arcu ultrices non. Aenean sit amet vestibulum sit\\nLorem sed diam purus, aliquam ac diam feugiat, volutpat lobortis ex cras nec sem\\nEducation\\n2013 - 2016 Masters Degree, Computer Science The University\\nThesis: Vestibulum Vel Lorem Ex Duis Varius Lorem Iaculis Laoreet Fringilla Nam Ex\\nDignissim malesuada vestibulum sed eget elit justo aliquam eu arcu a dui interdum eget\\nDonec mattis, purus vel pellentesque maximus tellus arcu tempus elit, ut sodales felis\\nElementum mauris arcu felis, sodales et maximus in, pretium maximus urna praesent\\n2009 - 2013 Bachelors Degree, Computer Science The University\\nThesis: Duis Molestie Faucibus Ligula, Sed Suscipit Tellus Tempor Sed Maecenas Li\\nAccumsan ligula at feugiat donec gravida, odio ac sodales consectetur, quam metus\\nMolestie mi, nec cursus sem leo sit amet sapien proin feugiat efficitur suscipit duis\\nhttps:/www.linkedin.com/in/john-doe  https:/ /github.com/john-doe')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b43d6678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73274aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.13.5-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->bs4) (4.14.1)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading beautifulsoup4-4.13.5-py3-none-any.whl (105 kB)\n",
      "Downloading soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "\n",
      "   ---------------------------------------- 0/3 [soupsieve]\n",
      "   ---------------------------------------- 0/3 [soupsieve]\n",
      "   ------------- -------------------------- 1/3 [beautifulsoup4]\n",
      "   ------------- -------------------------- 1/3 [beautifulsoup4]\n",
      "   ------------- -------------------------- 1/3 [beautifulsoup4]\n",
      "   ------------- -------------------------- 1/3 [beautifulsoup4]\n",
      "   ------------- -------------------------- 1/3 [beautifulsoup4]\n",
      "   -------------------------- ------------- 2/3 [bs4]\n",
      "   ---------------------------------------- 3/3 [bs4]\n",
      "\n",
      "Successfully installed beautifulsoup4-4.13.5 bs4-0.0.2 soupsieve-2.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ea550b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = WebBaseLoader(web_path=\"https://www.geeksforgeeks.org/artificial-intelligence/large-language-model-llm/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd8c12b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.cloudflare.com/learning/ai/what-is-large-language-model/', 'title': 'Just a moment...', 'language': 'en-US'}, page_content='Just a moment...Enable JavaScript and cookies to continue\\n')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test3.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beff8721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.geeksforgeeks.org/artificial-intelligence/large-language-model-llm/', 'title': 'What is a Large Language Model (LLM) - GeeksforGeeks', 'description': 'Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.', 'language': 'en-US'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is a Large Language Model (LLM) - GeeksforGeeks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCoursesDSA / PlacementsGATE 2026 PrepML & Data ScienceDevelopmentCloud / DevOpsProgramming LanguagesAll CoursesTutorialsPythonJavaDSAML & Data ScienceInterview CornerProgramming LanguagesWeb DevelopmentGATECS SubjectsDevOpsSchool LearningSoftware and ToolsPracticePractice Coding ProblemsNation Skillup- Free CoursesProblem of the DayJobsBecome a MentorApply Now!Post JobsJob-A-Thon: Hiring ChallengeJobs Updates\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotifications\\n\\nMark all as read\\n\\n\\n\\n\\n\\nAll\\n\\n\\n\\n\\n\\n \\n\\nView All\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotifications\\n\\n\\n\\nMark all as read\\n\\n\\n\\n\\n\\n\\nAll\\n\\n\\nUnread\\n\\n\\nRead\\n\\n\\n\\n\\n\\r\\n                        You're all caught up!!\\r\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPythonR LanguagePython for Data ScienceNumPyPandasOpenCVData AnalysisML MathMachine LearningNLPDeep LearningDeep Learning Interview QuestionsMachine LearningML ProjectsML Interview Questions \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign In\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOpen In App\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNation SkillUpShare Your ExperiencesIntroduction to AIWhat is Artificial Intelligence (AI)Types of Artificial Intelligence (AI)Types of AI Based on FunctionalitiesAgents in AIArtificial intelligence vs Machine Learning vs Deep LearningProblem Solving in Artificial IntelligenceTop 20 Applications of Artificial Intelligence (AI) in 2025AI ConceptsSearch Algorithms in AILocal Search Algorithm in Artificial IntelligenceAdversarial Search Algorithms in Artificial Intelligence (AI)Constraint Satisfaction Problems (CSP) in Artificial IntelligenceKnowledge Representation in AIFirst-Order Logic in Artificial IntelligenceReasoning Mechanisms in AIMachine Learning in AIMachine Learning TutorialDeep Learning TutorialNatural Language Processing (NLP) TutorialComputer Vision TutorialRobotics and AIArtificial Intelligence in RoboticsWhat is Robotics Process AutomationAutomated Planning in AIAI in Transportation - Benifits, Use Cases and ExamplesAI in Manufacturing : Revolutionizing the IndustryGenerative AIWhat is Generative AI?Generative Adversarial Network (GAN)Cycle Generative Adversarial Network (CycleGAN)StyleGAN - Style Generative Adversarial NetworksIntroduction to Generative Pre-trained Transformer (GPT)BERT Model - NLPGenerative AI Applications AI PracticeTop Artificial Intelligence(AI) Interview Questions and Answers\\tTop Generative AI Interview Question with Answer30+ Best Artificial Intelligence Project Ideas with Source Code [2025 Updated]DSA to Development Course \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is a Large Language Model (LLM)\\n\\n\\n\\nLast Updated : \\n21 Aug, 2025\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nComments\\n\\n\\n\\n\\n\\n\\n\\nImprove\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest changes\\n\\n\\n\\n\\n\\n\\nLike Article\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReport\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLarge Language Models (LLMs) are advanced AI systems based on deep neural networks, designed to process and generate text that resembles human language. They form the backbone of modern Natural Language Processing (NLP) applications, powering tools like ChatGPT, Google Gemini and Anthropic Claude.LLMTypical applications include:Text generation & summarizationConversational AI (chatbots, assistants)Language translation & correctionCode generation & debuggingMultimodal tasks (image + text reasoning, video understanding)  Evolution of GPT Models (by OpenAI)GPT-1 which was released in 2018 contains 117 million parameters having 985 million words.GPT-2 which was released in 2019 contains 1.5 billion parameters.GPT-3 which was released in 2020 contains 175 billion parameters. Chat GPT is also based on this model as well.GPT-4 model is released in the early 2023 and it is likely to contain trillions of parameters.GPT-4 Turbo was introduced in late 2023, optimized for speed and cost-efficiency, but its parameter count remains unspecified.How do Large Language Models work?LLMs use the Transformer architecture, which relies on:Input embeddings: Convert words/subwords into vectors.Positional encoding: Adds order information.Self-attention: Learns relationships between tokens.Feed-forward layers: Capture complex interactions.Decoder (for generation): Produces output step-by-step.Multi-head attention: Enables parallel reasoning over different relationships.Architecture of LLMLarge Language Model's (LLM) architecture is determined by a number of factors, like the objective of\\xa0the specific model design, the available computational resources and the kind of language processing tasks that are to be carried out by the LLM. The general architecture of LLM consists of many layers such as the feed forward layers, embedding layers, attention layers. A text which is embedded inside is collaborated together to generate predictions.Important components to influence Large Language Model architecture:Model Size and Parameter Countinput representationsSelf-Attention MechanismsTraining ObjectivesComputational EfficiencyDecoding and Output GenerationTransformer-Based LLM Model ArchitecturesTransformer-based models, which have revolutionized natural language processing tasks, typically follow a general architecture that includes the following components:1. Input Embeddings:\\xa0The input text is tokenized into smaller units, such as words or sub-words and each token is embedded into a continuous vector representation. This embedding step captures the semantic and syntactic information of the input.2. Positional Encoding:\\xa0Positional encoding is added to the input embeddings to provide information about the positions of the tokens because transformers do not naturally encode the order of the tokens. This enables the model to process the tokens while taking their sequential order into account.3. Encoder:\\xa0Based on a neural network technique, the encoder analyses the input text and creates a number of hidden states that protect the context and meaning of text data. Multiple encoder layers make up the core of the transformer architecture. Self-attention mechanism and feed-forward neural network are the two fundamental sub-components of each encoder layer.Self-Attention Mechanism:\\xa0Self-attention enables the model to weigh the importance of different tokens in the input sequence by computing attention scores. It allows the model to consider the dependencies and relationships between different tokens in a context-aware manner.Feed-Forward Neural Network:\\xa0After the self-attention step, a feed-forward neural network is applied to each token independently. This network includes fully connected layers with non-linear activation functions, allowing the model to capture complex interactions between tokens.4. Decoder Layers:\\xa0In some transformer-based models, a decoder component is included in addition to the encoder. The decoder layers enable autoregressive generation, where the model can generate sequential outputs by attending to the previously generated tokens.5. Multi-Head Attention:\\xa0Transformers often employ multi-head attention, where self-attention is performed simultaneously with different learned attention weights. This allows the model to capture different types of relationships and attend to various parts of the input sequence simultaneously.6. Layer Normalization:\\xa0Layer normalization is applied after each sub-component or layer in the transformer architecture. It helps stabilize the learning process and improves the model's ability to generalize across different inputs.7. Output Layers:\\xa0The output layers of the transformer model can vary depending on the specific task. For example, in language modeling, a linear projection followed by SoftMax activation is commonly used to generate the probability distribution over the next token.Popular Large Language Models Now let's look at some of the famous LLMs which has been developed and are up for inference.GPT-4 & GPT-4o (OpenAI): Advanced multimodal reasoning.Gemini 1.5 (Google DeepMind): Long-context reasoning (can process 1M+ tokens).Claude 3 (Anthropic): Safety-focused, strong in reasoning and summarization.LLaMA 3 (Meta): Open-weight model widely used in research & startups.Mistral 7B / Mixtral (Mistral AI): Efficient open-source alternatives.BERT / RoBERTa (Google/Facebook): Strong at embeddings and understanding tasks.mBERT & XLM-R: Early multilingual LLMs (contrary to claims that BLOOM was first).BLOOM: Collaborative open-source multilingual model (but not the first).Large Language Models Use CasesCode Generation: LLMs can generate accurate code based on user instructions for specific tasks.Debugging and Documentation: They assist in identifying code errors, suggesting fixes and even automating project documentation.Question Answering: Users can ask both casual and complex questions, receiving detailed, context-aware responses.Language Translation and Correction: LLMs can translate text between over 50 languages and correct grammatical errors.Prompt-Based Versatility: By crafting creative prompts, users can unlock endless possibilities, as LLMs excel in one-shot and zero-shot learning scenarios. Difference Between NLP and LLM\\xa0NLP is Natural Language Processing, a field of artificial intelligence (AI). It consists of the development of the algorithms. NLP is a broader field than LLM, which consists of algorithms and techniques. NLP rules two approaches i.e. Machine learning and the analyze language data. Applications of NLP are-Automotive routine taskImprove search\\xa0Search engine optimizationAnalyzing and organizing large documentsSocial Media Analytics.while on the other hand, LLM is a Large Language Model and is more specific to human- like text, providing content generation and personalized recommendations.\\xa0Advantages of Large Language ModelsLarge Language Models (LLMs) come with several advantages that contribute to their widespread adoption and success in various applications:LLMs can perform zero-shot learning, meaning they can generalize to tasks for which they were not explicitly trained. This capability allows for adaptability to new applications and scenarios without additional training.LLMs efficiently handle vast amounts of data, making them suitable for tasks that require a deep understanding of extensive text corpora, such as language translation and document summarization.LLMs can be fine-tuned on specific datasets or domains, allowing for continuous learning and adaptation to specific use cases or industries.LLMs enable the automation of various language-related tasks, from code generation to content creation, freeing up human resources for more strategic and complex aspects of a project.Challenges in Training of Large Language ModelsHigh Costs: Training LLMs requires significant financial investment, with millions of dollars needed for large-scale computational power.Time-Intensive: Training takes months, often involving human intervention for fine-tuning to achieve optimal performance.Data Challenges: Obtaining large text datasets is difficult and concerns about the legality of data scraping for commercial purposes have arisen.Environmental Impact: Training a single LLM from scratch can produce carbon emissions equivalent to the lifetime emissions of five cars, raising serious environmental concerns. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n        Comment\\r\\n    More infoAdvertise with us \\n\\n\\n\\n\\n\\n\\nA\\n\\n\\n\\n\\n \\n\\nabhishekm482g \\n\\n\\n\\n\\n\\n Follow \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nImprove\\n\\n\\n\\n\\n\\n\\nArticle Tags : \\n\\n\\nArtificial Intelligence\\n\\n\\ndata-science\\n\\n\\nChatGPT\\n \\n\\n\\n\\n\\n\\n\\n\\n\\nLike\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n864 interested Geeks \\n\\n\\n\\nGATE DA 2028 Online Course [Live Classes] \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2k+ interested Geeks \\n\\n\\n\\nGATE CS/IT 2028 Complete Course [with Placement Preparation] \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1k+ interested Geeks \\n\\n\\n\\nGATE CSE + DA 2027 Live Course [with Placement Preparation] \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCorporate & Communications Address:\\n\\r\\n                      A-143, 7th Floor, Sovereign Corporate Tower, Sector- 136, Noida, Uttar Pradesh (201305)                    \\n\\n\\n\\n\\n\\nRegistered Address:\\r\\n                        K 061, Tower K, Gulshan Vivante Apartment, Sector 137, Noida, Gautam Buddh Nagar, Uttar Pradesh, 201305                      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAdvertise with us\\n\\n\\n\\n\\nCompanyAbout UsLegalPrivacy PolicyCareersContact UsCorporate SolutionCampus Training ProgramExplorePOTDJob-A-ThonConnectCommunityBlogsNation Skill UpTutorialsProgramming LanguagesDSAWeb TechnologyAI, ML & Data ScienceDevOpsCS Core SubjectsInterview PreparationGATESchool SubjectsSoftware and ToolsCoursesIBM CertificationDSA and PlacementsWeb DevelopmentData ScienceProgramming LanguagesDevOps & CloudGATETrending TechnologiesOffline CentersNoidaBengaluruPuneHyderabadPatnaPreparation CornerAptitudePuzzlesGfG 160DSA 360System Design \\n\\n\\n\\n\\n\\n\\n@GeeksforGeeks, Sanchhaya Education Private Limited, All rights reserved\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nImprovement\\n\\n\\n\\n\\n\\n\\n\\nSuggest changes\\n\\n\\n\\n\\n\\nSuggest Changes\\nHelp us improve. Share your suggestions to enhance the article. Contribute your expertise and make a difference in the GeeksforGeeks portal.\\n\\n\\n\\n\\n\\n\\n\\nCreate Improvement\\nEnhance the article with your expertise. Contribute to the GeeksforGeeks community and help create better learning resources for all.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest Changes\\n\\n\\n\\n\\n\\n\\nmin 4 words, max Words Limit:1000\\n\\n\\n\\n\\nThank You!\\nYour suggestions are valuable to us.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat kind of Experience do you want to share?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nInterview Experiences\\n\\n\\n\\n\\n\\n\\n\\nAdmission Experiences\\n\\n\\n\\n\\n\\n\\n\\nCareer Journeys\\n\\n\\n\\n\\n\\n\\n\\nWork Experiences\\n\\n\\n\\n\\n\\n\\n\\nCampus Experiences\\n\\n\\n\\n\\n\\n\\n\\nCompetitive Exam Experiences\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test3.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c485e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56d1e8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ArxivLoader\n",
      "  Downloading arxivloader-1.0.2.tar.gz (10 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: bs4 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ArxivLoader) (0.0.2)\n",
      "Collecting lxml (from ArxivLoader)\n",
      "  Downloading lxml-6.0.1-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ArxivLoader) (2.2.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ArxivLoader) (2.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ArxivLoader) (2.32.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ArxivLoader) (4.67.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bs4->ArxivLoader) (4.13.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->bs4->ArxivLoader) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->bs4->ArxivLoader) (4.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vijay\\appdata\\roaming\\python\\python311\\site-packages (from pandas->ArxivLoader) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->ArxivLoader) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->ArxivLoader) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vijay\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->ArxivLoader) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->ArxivLoader) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->ArxivLoader) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->ArxivLoader) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->ArxivLoader) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\vijay\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->ArxivLoader) (0.4.6)\n",
      "Downloading lxml-6.0.1-cp311-cp311-win_amd64.whl (4.0 MB)\n",
      "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "   -------------------------------------- - 3.9/4.0 MB 29.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.0/4.0 MB 20.2 MB/s  0:00:00\n",
      "Building wheels for collected packages: ArxivLoader\n",
      "  Building wheel for ArxivLoader (pyproject.toml): started\n",
      "  Building wheel for ArxivLoader (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for ArxivLoader: filename=arxivloader-1.0.2-py3-none-any.whl size=9212 sha256=5b624abc3471c6e2bfb0293d145dfc2871be600b0a07c06ce91dc8f4eadfe1ac\n",
      "  Stored in directory: c:\\users\\vijay\\appdata\\local\\pip\\cache\\wheels\\ef\\41\\84\\a6f90966fb2d64c1fd4dc8cad5a4b1bc9bf66a6284eca7170c\n",
      "Successfully built ArxivLoader\n",
      "Installing collected packages: lxml, ArxivLoader\n",
      "\n",
      "   ---------------------------------------- 0/2 [lxml]\n",
      "   ---------------------------------------- 0/2 [lxml]\n",
      "   ---------------------------------------- 0/2 [lxml]\n",
      "   ---------------------------------------- 0/2 [lxml]\n",
      "   ---------------------------------------- 0/2 [lxml]\n",
      "   ---------------------------------------- 0/2 [lxml]\n",
      "   ---------------------------------------- 0/2 [lxml]\n",
      "   ---------------------------------------- 2/2 [ArxivLoader]\n",
      "\n",
      "Successfully installed ArxivLoader-1.0.2 lxml-6.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ArxivLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d73496e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import arxiv python package. Please install it with `pip install arxiv`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vijay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\utilities\\arxiv.py:81\u001b[39m, in \u001b[36mArxivAPIWrapper.validate_environment\u001b[39m\u001b[34m(cls, values)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01marxiv\u001b[39;00m\n\u001b[32m     83\u001b[39m     values[\u001b[33m\"\u001b[39m\u001b[33marxiv_search\u001b[39m\u001b[33m\"\u001b[39m] = arxiv.Search\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'arxiv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m test4 = \u001b[43mArxivLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1706.03762\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vijay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\document_loaders\\arxiv.py:143\u001b[39m, in \u001b[36mArxivLoader.__init__\u001b[39m\u001b[34m(self, query, doc_content_chars_max, **kwargs)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Initialize with search query to find documents in the Arxiv.\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[33;03mSupports all arguments of `ArxivAPIWrapper`.\u001b[39;00m\n\u001b[32m    136\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m \u001b[33;03m    doc_content_chars_max: cut limit for the length of a document's content\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[38;5;28mself\u001b[39m.query = query\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m \u001b[38;5;28mself\u001b[39m.client = \u001b[43mArxivAPIWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoc_content_chars_max\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoc_content_chars_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vijay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\utilities\\arxiv.py:91\u001b[39m, in \u001b[36mArxivAPIWrapper.validate_environment\u001b[39m\u001b[34m(cls, values)\u001b[39m\n\u001b[32m     89\u001b[39m     values[\u001b[33m\"\u001b[39m\u001b[33marxiv_result\u001b[39m\u001b[33m\"\u001b[39m] = arxiv.Result\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     92\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import arxiv python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     93\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install arxiv`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     94\u001b[39m     )\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[31mImportError\u001b[39m: Could not import arxiv python package. Please install it with `pip install arxiv`."
     ]
    }
   ],
   "source": [
    "test4 = ArxivLoader(query=\"1706.03762\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e0a543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ArxivLoader in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: bs4 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ArxivLoader) (0.0.2)\n",
      "Requirement already satisfied: lxml in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ArxivLoader) (6.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ArxivLoader) (2.2.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ArxivLoader) (2.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ArxivLoader) (2.32.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ArxivLoader) (4.67.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bs4->ArxivLoader) (4.13.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->bs4->ArxivLoader) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->bs4->ArxivLoader) (4.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vijay\\appdata\\roaming\\python\\python311\\site-packages (from pandas->ArxivLoader) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->ArxivLoader) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->ArxivLoader) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vijay\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->ArxivLoader) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->ArxivLoader) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->ArxivLoader) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->ArxivLoader) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->ArxivLoader) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\vijay\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->ArxivLoader) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ArxivLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4263dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe208f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e6ca91c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import arxiv python package. Please install it with `pip install arxiv`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vijay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\utilities\\arxiv.py:81\u001b[39m, in \u001b[36mArxivAPIWrapper.validate_environment\u001b[39m\u001b[34m(cls, values)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01marxiv\u001b[39;00m\n\u001b[32m     83\u001b[39m     values[\u001b[33m\"\u001b[39m\u001b[33marxiv_search\u001b[39m\u001b[33m\"\u001b[39m] = arxiv.Search\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'arxiv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m test5 = \u001b[43mArxivLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1706.03762\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vijay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\document_loaders\\arxiv.py:143\u001b[39m, in \u001b[36mArxivLoader.__init__\u001b[39m\u001b[34m(self, query, doc_content_chars_max, **kwargs)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Initialize with search query to find documents in the Arxiv.\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[33;03mSupports all arguments of `ArxivAPIWrapper`.\u001b[39;00m\n\u001b[32m    136\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m \u001b[33;03m    doc_content_chars_max: cut limit for the length of a document's content\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[38;5;28mself\u001b[39m.query = query\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m \u001b[38;5;28mself\u001b[39m.client = \u001b[43mArxivAPIWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoc_content_chars_max\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoc_content_chars_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vijay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\utilities\\arxiv.py:91\u001b[39m, in \u001b[36mArxivAPIWrapper.validate_environment\u001b[39m\u001b[34m(cls, values)\u001b[39m\n\u001b[32m     89\u001b[39m     values[\u001b[33m\"\u001b[39m\u001b[33marxiv_result\u001b[39m\u001b[33m\"\u001b[39m] = arxiv.Result\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     92\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import arxiv python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     93\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install arxiv`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     94\u001b[39m     )\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[31mImportError\u001b[39m: Could not import arxiv python package. Please install it with `pip install arxiv`."
     ]
    }
   ],
   "source": [
    "test5 = ArxivLoader(query = \"1706.03762\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e22c6cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Arxiv\n",
      "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Arxiv) (6.0.11)\n",
      "Requirement already satisfied: requests~=2.32.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Arxiv) (2.32.5)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from feedparser~=6.0.10->Arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests~=2.32.0->Arxiv) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests~=2.32.0->Arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests~=2.32.0->Arxiv) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests~=2.32.0->Arxiv) (2025.8.3)\n",
      "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: Arxiv\n",
      "Successfully installed Arxiv-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Arxiv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d392072",
   "metadata": {},
   "outputs": [],
   "source": [
    "test5 = ArxivLoader(query = \"1706.03762\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "342ebe26",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "PyMuPDF package not found, please install it with `pip install pymupdf`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vijay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\utilities\\arxiv.py:201\u001b[39m, in \u001b[36mArxivAPIWrapper.lazy_load\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfitz\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'fitz'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtest5\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vijay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:43\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m     38\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load data into Document objects.\u001b[39;00m\n\u001b[32m     39\u001b[39m \n\u001b[32m     40\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m        the documents.\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vijay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\document_loaders\\arxiv.py:149\u001b[39m, in \u001b[36mArxivLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlazy_load\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[Document]:\n\u001b[32m    148\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Lazy load Arvix documents\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.lazy_load(\u001b[38;5;28mself\u001b[39m.query)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vijay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\utilities\\arxiv.py:203\u001b[39m, in \u001b[36mArxivAPIWrapper.lazy_load\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfitz\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    204\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPyMuPDF package not found, please install it with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    205\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`pip install pymupdf`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    206\u001b[39m     )\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    209\u001b[39m     \u001b[38;5;66;03m# Remove the \":\" and \"-\" from the query, as they can cause search problems\u001b[39;00m\n\u001b[32m    210\u001b[39m     query = query.replace(\u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).replace(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: PyMuPDF package not found, please install it with `pip install pymupdf`"
     ]
    }
   ],
   "source": [
    "test5.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02614c6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to function call here. Maybe you meant '==' instead of '='? (2852139977.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtest5() = ArxivLoader(query=\"1706.03762\")\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m cannot assign to function call here. Maybe you meant '==' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "test5() = ArxivLoader(query=\"1706.03762\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2318e5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Arxiv in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Arxiv) (6.0.11)\n",
      "Requirement already satisfied: requests~=2.32.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Arxiv) (2.32.5)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from feedparser~=6.0.10->Arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests~=2.32.0->Arxiv) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests~=2.32.0->Arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests~=2.32.0->Arxiv) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests~=2.32.0->Arxiv) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Arxiv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0da8be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "68d1bb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "test6=ArxivLoader(query=\"1706.03762\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f498ccfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit\\nGoogle Research\\nusz@google.com\\nLlion Jones\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nukasz Kaiser\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\nEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\nWork performed while at Google Brain.\\nWork performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\ndk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\ndk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\ndk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q  k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\nRdmodeldk, W K\\ni\\nRdmodeldk, W V\\ni\\nRdmodeldv\\nand W O Rhdvdmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to ) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by dmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2  d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n  d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k  n  d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r  n  d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2 to 10000  2. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k  n  d + n  d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with 1 = 0.9, 2 = 0.98 and  = 109. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d0.5\\nmodel  min(step_num0.5, step_num  warmup_steps1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0  1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3  1019\\n1.4  1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6  1018\\n1.5  1020\\nMoE [32]\\n26.03\\n40.56\\n2.0  1019\\n1.2  1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0  1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8  1020\\n1.1  1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7  1019\\n1.2  1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3  1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3  1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty  = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and  = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, aglar Glehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jrgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jrgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):17351780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Tckstrm, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):19291958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 24402448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 31043112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Googles neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434443. ACL, August 2013.\\n12\\nAttention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb making, completing the phrase making...more difficult. Attentions here shown only for\\nthe word making. Different colors represent different heads. Best viewed in color.\\n13\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word its for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test6.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd9f575c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.26.4-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.26.4-cp39-abi3-win_amd64.whl (18.7 MB)\n",
      "   ---------------------------------------- 0.0/18.7 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.0/18.7 MB 5.6 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 2.4/18.7 MB 5.8 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 3.7/18.7 MB 5.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 5.0/18.7 MB 6.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 6.6/18.7 MB 6.3 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 8.1/18.7 MB 6.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 9.7/18.7 MB 6.5 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 11.3/18.7 MB 6.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 12.6/18.7 MB 6.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 13.6/18.7 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 14.9/18.7 MB 6.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 16.5/18.7 MB 6.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.8/18.7 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.6/18.7 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.7/18.7 MB 6.1 MB/s  0:00:03\n",
      "Installing collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyMuPDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dd42cecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test6 = ArxivLoader(query=\"1706.03762\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "597ba370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit\\nGoogle Research\\nusz@google.com\\nLlion Jones\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nukasz Kaiser\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\nEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\nWork performed while at Google Brain.\\nWork performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\ndk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\ndk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\ndk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q  k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\nRdmodeldk, W K\\ni\\nRdmodeldk, W V\\ni\\nRdmodeldv\\nand W O Rhdvdmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to ) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by dmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2  d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n  d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k  n  d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r  n  d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2 to 10000  2. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k  n  d + n  d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with 1 = 0.9, 2 = 0.98 and  = 109. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d0.5\\nmodel  min(step_num0.5, step_num  warmup_steps1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0  1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3  1019\\n1.4  1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6  1018\\n1.5  1020\\nMoE [32]\\n26.03\\n40.56\\n2.0  1019\\n1.2  1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0  1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8  1020\\n1.1  1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7  1019\\n1.2  1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3  1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3  1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty  = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and  = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, aglar Glehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jrgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jrgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):17351780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Tckstrm, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):19291958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 24402448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 31043112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Googles neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434443. ACL, August 2013.\\n12\\nAttention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb making, completing the phrase making...more difficult. Attentions here shown only for\\nthe word making. Different colors represent different heads. Best viewed in color.\\n13\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word its for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test6.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "468f056e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test6' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m docs = \u001b[43mtest6\u001b[49m.load()\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtest6_output.pdf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n",
      "\u001b[31mNameError\u001b[39m: name 'test6' is not defined"
     ]
    }
   ],
   "source": [
    "docs = test6.load()\n",
    "with open(\"test6_output.pdf\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for doc in docs:\n",
    "        f.write(doc.page_content + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca6f4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c604a04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ArxivLoader in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: bs4 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ArxivLoader) (0.0.2)\n",
      "Requirement already satisfied: lxml in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ArxivLoader) (6.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ArxivLoader) (2.2.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ArxivLoader) (2.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ArxivLoader) (2.32.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ArxivLoader) (4.67.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bs4->ArxivLoader) (4.13.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->bs4->ArxivLoader) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->bs4->ArxivLoader) (4.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vijay\\appdata\\roaming\\python\\python311\\site-packages (from pandas->ArxivLoader) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->ArxivLoader) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->ArxivLoader) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vijay\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->ArxivLoader) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->ArxivLoader) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->ArxivLoader) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->ArxivLoader) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->ArxivLoader) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\vijay\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->ArxivLoader) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ArxivLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e81b070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "46cf1590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: requests~=2.32.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from arxiv) (2.32.5)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests~=2.32.0->arxiv) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests~=2.32.0->arxiv) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bcce4dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test8=ArxivLoader(query=\"1706.03762\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "51f77053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit\\nGoogle Research\\nusz@google.com\\nLlion Jones\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nukasz Kaiser\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\nEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\nWork performed while at Google Brain.\\nWork performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\ndk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\ndk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\ndk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q  k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\nRdmodeldk, W K\\ni\\nRdmodeldk, W V\\ni\\nRdmodeldv\\nand W O Rhdvdmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to ) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by dmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2  d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n  d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k  n  d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r  n  d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2 to 10000  2. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k  n  d + n  d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with 1 = 0.9, 2 = 0.98 and  = 109. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d0.5\\nmodel  min(step_num0.5, step_num  warmup_steps1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0  1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3  1019\\n1.4  1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6  1018\\n1.5  1020\\nMoE [32]\\n26.03\\n40.56\\n2.0  1019\\n1.2  1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0  1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8  1020\\n1.1  1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7  1019\\n1.2  1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3  1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3  1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty  = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and  = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, aglar Glehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jrgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jrgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):17351780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Tckstrm, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):19291958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 24402448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 31043112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Googles neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434443. ACL, August 2013.\\n12\\nAttention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb making, completing the phrase making...more difficult. Attentions here shown only for\\nthe word making. Different colors represent different heads. Best viewed in color.\\n13\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word its for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test8.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a787e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5788a7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit\\nGoogle Research\\nusz@google.com\\nLlion Jones\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nukasz Kaiser\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\nEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\nWork performed while at Google Brain.\\nWork performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\ndk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\ndk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\ndk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q  k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\nRdmodeldk, W K\\ni\\nRdmodeldk, W V\\ni\\nRdmodeldv\\nand W O Rhdvdmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to ) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by dmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2  d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n  d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k  n  d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r  n  d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2 to 10000  2. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k  n  d + n  d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with 1 = 0.9, 2 = 0.98 and  = 109. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d0.5\\nmodel  min(step_num0.5, step_num  warmup_steps1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0  1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3  1019\\n1.4  1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6  1018\\n1.5  1020\\nMoE [32]\\n26.03\\n40.56\\n2.0  1019\\n1.2  1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0  1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8  1020\\n1.1  1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7  1019\\n1.2  1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3  1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3  1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty  = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and  = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, aglar Glehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jrgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jrgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):17351780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Tckstrm, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):19291958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 24402448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 31043112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Googles neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434443. ACL, August 2013.\\n12\\nAttention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb making, completing the phrase making...more difficult. Attentions here shown only for\\nthe word making. Different colors represent different heads. Best viewed in color.\\n13\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word its for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test8.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c4fda5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test9 = ArxivLoader(query=\"2509.10427\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "24811690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2025-09-12', 'Title': 'My Favorite Streamer is an LLM: Discovering, Bonding, and Co-Creating in AI VTuber Fandom', 'Authors': 'Jiayi Ye, Chaoran Chen, Yue Huang, Yanfang Ye, Toby Jia-Jun Li, Xiangliang Zhang', 'Summary': \"AI VTubers, where the performer is not human but algorithmically generated,\\nintroduce a new context for fandom. While human VTubers have been substantially\\nstudied for their cultural appeal, parasocial dynamics, and community\\neconomies, little is known about how audiences engage with their AI\\ncounterparts. To address this gap, we present a qualitative study of\\nNeuro-sama, the most prominent AI VTuber. Our findings show that engagement is\\nanchored in active co-creation: audiences are drawn by the AI's unpredictable\\nyet entertaining interactions, cement loyalty through collective emotional\\nevents that trigger anthropomorphic projection, and sustain attachment via the\\nAI's consistent persona. Financial support emerges not as a reward for\\nperformance but as a participatory mechanism for shaping livestream content,\\nestablishing a resilient fan economy built on ongoing interaction. These\\ndynamics reveal how AI Vtuber fandom reshapes fan-creator relationships and\\noffer implications for designing transparent and sustainable AI-mediated\\ncommunities.\"}, page_content='My Favorite Streamer is an LLM: Discovering, Bonding, and Co-Creating in AI\\nVTuber Fandom\\nJIAYI YE, Independent Researcher, China\\nCHAORAN CHEN, University of Notre Dame, United States\\nYUE HUANG, University of Notre Dame, United States\\nYANFANG YE, University of Notre Dame, United States\\nTOBY JIA-JUN LI, University of Notre Dame, United States\\nXIANGLIANG ZHANG, University of Notre Dame, United States\\nAI VTubers, where the performer is not human but algorithmically generated, introduce a new context for fandom. While human\\nVTubers have been substantially studied for their cultural appeal, parasocial dynamics, and community economies, little is known\\nabout how audiences engage with their AI counterparts. To address this gap, we present a qualitative study of Neuro-sama, the\\nmost prominent AI VTuber. Our findings show that engagement is anchored in active co-creation: audiences are drawn by the\\nAIs unpredictable yet entertaining interactions, cement loyalty through collective emotional events that trigger anthropomorphic\\nprojection, and sustain attachment via the AIs consistent persona. Financial support emerges not as a reward for performance but as a\\nparticipatory mechanism for shaping livestream content, establishing a resilient fan economy built on ongoing interaction. These\\ndynamics reveal how AI Vtuber fandom reshapes fancreator relationships and offer implications for designing transparent and\\nsustainable AI-mediated communities.\\nCCS Concepts:  Human-centered computing Empirical studies in collaborative and social computing.\\nAdditional Key Words and Phrases: Virtual YouTuber, Human-AI Interaction, Livestreaming\\n1\\nIntroduction\\nIn late 2024, the AI VTuber Neuro-sama1 was nominated for VTuber of the Year [53]. This milestone signals that\\nAI VTubers have entered mainstream recognition. For HCI and media studies, it raises an urgent question: when the\\nperformer is an AI system, do audiences engage in ways continuous with human VTuber cultures [8, 35, 46, 57], or do\\nnew dynamics emerge that reshape how participation, authenticity, and monetization are experienced?\\nVTubers are online performers who use avatars in livestreams, blending authenticity and performance [27]. AI\\nVTubers (Fig.1) represent a distinct subset whose dialogue and behavior are generated primarily by large language\\nmodels (LLMs) or related generative systems, rather than by a human performer. Prior work has examined AI in\\nlivestreaming primarily as a tool for commerce [4, 23, 34, 59, 62, 64, 66] or as supportive assistance to human streamers\\n[17, 40]. Other work has explored technical feasibility through prototypes such as Bilibug [32] or KawAIi [1]. What\\nremains missing is an account of fully autonomous, LLM-driven entertainers: why humans are drawn to them, how\\nparasocial bonds and community identities form when the agent is visibly non-human, and what economic logics\\nstabilize around such engagements. Technical feasibility [6, 60] explains that an AI VTuber can exist, but it does not\\nexplain why audiences stay, attach, and pay.\\nIn this context, studying AI VTubers carries both theoretical and practical significance. Theoretically, it pushes media\\nand HCI scholarship to revisit foundational concepts of participation, authenticity, and fan economy in contexts where\\nperformers are not human. Practically, it surfaces design challenges for AI-mediated entertainment: balancing fairness\\nwith monetization, ensuring persona stability without losing unpredictability, and addressing risks of over-attachment.\\n1https://www.twitch.tv/vedal987\\n1\\narXiv:2509.10427v1  [cs.HC]  12 Sep 2025\\n2\\nYe, et al.\\n(a) Chats\\n(b) SuperChat\\n(c) AI VTuber Neuro-samas \\nLive2d avatar\\n(d) Live captions of Neuro-sama\\'s speech\\nFig. 1. Screenshot from a Neuro-sama livestream. (a) Live chat messages responding to the topic under discussion. (b) A SuperChat\\nintroducing a new question. (c) Neuro-samas Live2D avatar. (d) On-screen captions displaying her speech. These elements illustrate\\nhow real-time audience interaction, paid prompts, and AI-driven performance combine in the AI VTuber experience.\\nThese stakes underline the urgency of treating AI VTubers not as a curiosity, but as a testbed for broader questions\\nabout human-AI intimacy and platform economies.\\nTo address this gap, we conducted an in-depth investigation of the fan community surrounding Neuro-sama, one of\\nthe most prominent AI VTubers [53]. We focused on the following research questions (RQs):\\nRQ1: How do audiences discover AI VTubers, and what motivates initial attraction?\\nRQ2: How do fans construct parasocial relationships and shared community cultures around AI VTubers?\\nRQ3: What drives fans to provide financial support to AI VTubers?\\nFigure 2 shows our multi-phase qualitative design that strategically combined complementary methods to capture\\nbreadth, depth, and behavioral validation. We first conducted a survey with 334 Neuro-sama fans to capture broad\\nmotivations and engagement patterns, followed by semi-structured interviews with 12 dedicated fans to probe\\nparasocial bonds and community meanings, and finally a log analysis of livestream interaction to examine co-\\ncreative practices and financial support behaviors. This triangulated design strengthened the credibility of our findings\\nand yielded a comprehensive account of engagement with AI VTubers.\\nOur findings show that (1) audiences are initially attracted by the unpredictability and improvisational quality of\\ncommunity-AI interaction, (2) they construct a new form of parasocial relationship where consistency rather than\\nhumanness anchors authenticity, and (3) they financially support AI VTubers not only to express recognition but to\\npurchase opportunities for real-time co-performance. Taken together, these findings reframe AI VTubers not simply as a\\nMy Favorite Streamer is an LLM: Discovering, Bonding, and Co-Creating in AI VTuber Fandom\\n3\\nPhase I\\nSurvey\\nTo establish a foundational \\nunderstanding of fan motivations, \\nperceptions, and financial support \\n(RQ1, RQ2 and RQ3).\\nPhase II\\nSemi-structured \\nInterviews\\nTo explore the deep personal \\nnarratives and emotional experiences \\nthat contextualize survey findings (RQ1 \\nand RQ2).\\nInteraction\\nLog Analysis\\nTo observe how fan relationships and \\nmotivations manifest in real-time, \\nauthentic behaviors (RQ2 and RQ3).\\nPhase III\\nTo Add Qualitative Depth\\nFor Behavioral Validation\\nn=334,\\nFrom fan groups,\\nsocial media platforms,\\nand personal contacts \\nn=12,\\nMost are deeply engaged fans\\nFrom Twitch,\\n550k Chats and 838 SuperChats\\nFig. 2. Overview of our three-phase research design. Phase I used surveys to establish a broad understanding of fan motivations,\\nperceptions, and financial support (RQ13). Phase II added interviews to contextualize these findings with personal narratives and\\nemotional meanings (RQ12). Phase III analyzed livestream interaction logs to validate how relationships and motivations manifest\\nin real-time behavior (RQ23).\\ncontinuation of human VTuber practices, but as a case that reveals new paradigms of human-AI co-performance and\\nplatform governance. Audience agency is simultaneously expanded (through real-time co-creation) and commodified\\n(through monetized intervention), raising critical questions about fairness and inclusion in participatory culture.\\nAuthenticity in mediated relationships is reconceptualized, as fans privilege consistency and reliability over human\\nlikeness, signaling a shift in how intimacy with non-human agents is negotiated. Finally, these dynamics foreground\\ndesign and policy challenges: how to balance openness with revenue, how to engineer persona stability without\\nsuppressing improvisation, and how to mitigate risks of psychological over-attachment.\\nOur primary contributions include:\\nWe present the first systematic study of a fan community centered on a fully AI-driven VTuber, combining\\nsurveys, interviews, and log analysis to map the pathway from initial discovery to sustained loyalty, thereby\\nextending VTuber scholarship into non-human performers.\\nWe extend theories of parasocial interaction, authenticity, and participatory culture by conceptualizing consistency-\\nas-authenticity and real-time co-performance commodification, showing how fans reconcile technical awareness\\nwith emotional projection and how monetization reconfigures participation.\\nWe distill design implications for future AI-mediated entertainment: balancing fairness with revenue, engineering\\npersona consistency while retaining improvisational appeal, and embedding safeguards against parasocial\\nover-attachment.\\n2\\nRelated Work\\n2.1\\nVTubers in HCI\\nVirtual YouTubers (VTubers) are livestreamers who perform through virtual avatars, while the human performer behind\\nthe avatar is referred to as Nakanohito. Since Kizuna Ais debut in 2016 [25], VTubing has become one of the most\\nprevalent forms of streaming, particularly among audiences who value anime-inspired aesthetics. By December 2022,\\nmore than 20,000 VTubers were active worldwide [51], with the most popular ones amassing more than four million\\nfollowers [52].HCI researchers have approached this phenomenon from three angles.\\n4\\nYe, et al.\\nOne body of work examines the design of VTubers themselves. Studies investigate how VTubers construct their\\navatars and identities, including the role of gender expression [3, 54], cross-cultural aesthetics [42], and inclusive\\nrepresentation [41]. Others examine the production side, such as the tools and environments used for VTubing [24].\\nThese studies highlight how technical and aesthetic choices shape the mediated presence of VTubers.\\nA complementary stream focuses on the relationship between VTubers and their fans. Research shows that these\\nbonds are best understood through parasocial interaction [46], where fans construct deep emotional ties to performers.\\nSuch connections can provide comfort and companionship [8, 35], but they also leave fans vulnerable to emotional\\ndisruption when streamers retire or go on hiatus [27]. Fan practices extend beyond individual attachment to collective\\nactivities (e.g., concerts and shared rituals) that reinforce community identity [28].\\nFinally, researchers have investigated the economic infrastructures that sustain VTuber communities. Scholars\\ndocument how subscription models build long-term loyalty [44], while virtual gift-giving serves as a social signal that\\nenhances fan visibility and status [30, 31]. Recent work also examines platform-specific monetization tools such as\\nSuperchat [63, 65], showing how fans use highlighted paid messages not only for recognition but also as a means of\\nparticipation in livestreams. These studies show that financial support functions as a social practice within VTuber\\nfandoms, rather than as a simple transaction.\\nThis extensive body of work on human VTubers provides a strong foundation, but it leaves open questions of\\nhow these dynamics unfold when the performer is not human. Existing scholarship has richly theorized avatars, fan\\nrelationships, and monetization in human-led contexts, yet we still lack understanding of how these frameworks extend\\nwhen the performer is an AI agent. Our work takes up this challenge by using Neuro-sama as a case to examine how\\nhumanAI interaction reshapes parasociality, participation, and platform economies.\\n2.2\\nAI Applications in Livestreaming\\nScholars have increasingly examined how AI is being integrated into livestreaming, with most research concentrating\\non e-commerce contexts. Early work compared AI streamers with human hosts and found that virtual agents can\\nperform competitively in commercial settings [61, 62]. Subsequent studies demonstrated that AI agents can actively\\nshape purchasing decisions [4, 34] and even influence the broader dynamics of digital commerce [59]. Beyond functional\\nperformance, anthropomorphic design features [5] and subtle emotional cues such as smiling [14, 23] have been shown\\nto strengthen consumer engagement. These studies suggest that the effectiveness of AI streamers depends not only on\\nwhat they do, but also on how human-like and emotionally expressive they appear.\\nA second line of work explores AI as supportive technology for human streamers. For example, commercial livestream\\nassistants can provide interactive support during broadcasts [55], while personality-infused AI companions can enhance\\naudience connection [17]. Other systems enable cross-language interactions through real-time translation [47]. Yet\\nthese applications face clear limitations: weaknesses in sensory language can undermine perceived authenticity [21],\\nand service failures can trigger consumer disengagement [38]. This literature illustrates both the supportive potential\\nand the persistent challenges of AI in livestreaming environments.\\nBy contrast, research on entertainment-oriented AI streamers is still sparse. Prior studies examined technical\\nintegrations of AI into VTubers [60] or the role of digital human technologies in social media livestreaming [6], but\\nlittle attention has been paid to fully autonomous AI VTubers that function as entertainers in their own right. The\\nrise of fully autonomous AI VTubers like Neuro-sama foregrounds not just technical feasibility but also questions of\\naudience engagement, parasociality, and collective identity. Our study addresses this underexplored dimension of AI in\\nlivestreaming by shifting the focus from commerce and assistance to entertainment and community-building.\\nMy Favorite Streamer is an LLM: Discovering, Bonding, and Co-Creating in AI VTuber Fandom\\n5\\n2.3\\nTheoretical Lenses on Participation and Authenticity\\nBeyond prior empirical work on VTubers and AI streamers, we also turn to theoretical traditions. In particular,\\nparticipatory culture and mediated authenticity provide valuable lenses for interpreting how fan practices change when\\nthe performer is an AI system.\\nJenkins theory of participatory culture characterizes audiences as active contributors who remix, circulate, and\\nco-create media rather than consume passively [22]. In the VTuber context, this is evident in fan remixes, meme\\ncirculation, and event participation [28]. Critical scholarship, however, has emphasized that such participation is never\\npurely empowering: platforms often commodify it, turning fans creative labor into monetized value streams [16, 50].\\nThis tension between agency and commodification is central to our inquiry, raising the question of whether real-time\\ninteractions (e.g., paid prompts) simply extend participatory culture or signal a shift toward new forms of monetized\\nco-performance.\\nAuthenticity is a second anchor in understanding mediated relationships. Enli frames authenticity as a negotiated\\nachievement between audiences and media figures [12], while parasocial interaction theory conceptualizes how\\naudiences build one-sided bonds with media characters as if they were real partners [45]. In human VTubing,\\nauthenticity often hinges on the fragile boundary between the avatar persona and the human performer (Nakanohito)\\nbehind it [35]. AI VTubers, by contrast, have no such backstage, raising questions about how authenticity is redefined\\nwhen the performer is visibly non-human.\\nThese theoretical perspectives highlight why AI VTubers provide a distinctive case for HCI and media studies. They\\nallow us to ask not only how audiences adopt AI-driven performers, but also how participation, authenticity, and\\nparasociality must be re-examined when the performer is an artificial agent. In particular, they offer an opportunity to\\ntest and extend classic frameworks under conditions where non-humanness is not hidden but made explicit.\\n3\\nMethod\\nTo comprehensively investigate AI VTuber fans formation, preservation, and financial activities, we adopted a research\\ndesign primarily rooted in qualitative inquiry. We selected Neuro-sama as our representative AI VTuber subject because\\nit is the most prominent AI VTuber, substantially surpassing other AI VTubers in follower count2. Furthermore, Neuro-\\nsama maintains a significant fan base on both the Twitch and Bilibili platforms, broadcasting simultaneously in English\\nand Chinese, which enables coverage of VTuber fans from diverse cultural backgrounds.\\nAs shown in Figure 2, the research was conducted in three sequential stages3. First, to establish a broad, foundational\\nunderstanding across all three RQs, we conducted a large-scale qualitative survey. This initial stage allowed us\\nto identify the key motivations, relational dynamics, and financial drivers within the broad fan community. Next, to\\nexplore the in-depth personal narratives and emotional experiences that contextualize the survey findings, we conducted\\nsupplementary semi-structured interview with fans, providing insight into how dedicated fans make meaning\\nof their relationship with Neuro-sama. Finally, to compare these self-reported perceptions with real-time behaviors,\\nwe performed an analysis of livestream interaction log data, including Chats and Superchats. This allowed us to\\nobserve how fans interact and provide financial support in real-time, and to contrast these behaviors with those seen in\\nhuman VTuber communities (discussed in subsubsection 3.3.1).\\n2https://virtual-youtuber.userlocal.jp/office/aivtuber.\\n3This research protocol has been approved by the Institutional Review Board (IRB).\\n6\\nYe, et al.\\n3.1\\nSurvey\\nWe conducted a large-scale survey among Neuro-samas fans to understand their engagement motivations, perceived\\nrelationship with Neuro-sama, and direct reasons for financial support.\\n3.1.1\\nParticipants and Recruitment. To ensure 1) all participants were genuine Neuro-sama fans qualified to provide\\nrelevant insights for our study, and 2) the participants represented diverse backgrounds and communities, we imple-\\nmented a careful recruitment strategy. Specifically, we recruited participants through a public call for participation\\ndistributed via social media fan groups for Neuro-sama (e.g., Discord, QQa widely used Chinese messaging and\\nsocial media platform similar to WhatsApp), public social media platforms, and personal contacts. An initial screening\\nquestion confirmed that all respondents had watched Neuro-samas streams (e.g., Do you watch the AI-driven VTuber,\\nNeuro-sama? ). As compensation for survey participation, we randomly selected three respondents to receive an official\\nNeuro-sama plush toy ($29.99 each). The selection of this compensation was intentional, as it would primarily appeal to\\nthose familiar with Neuro-samas content, thus helping to ensure authentic participation from dedicated fans rather\\nthan casual respondents.\\nUltimately, we collected 334 valid responses. The sample demonstrated high engagement with Neuro-sama: over\\nhalf of the participants (52.69%) reported watching Neuro-samas streams or related videos almost daily, and a total of\\n77.24% watched at least three times a week. The sample was nearly evenly split between experienced and new VTuber\\nviewers; 48.8% were regular viewers of human VTubers before watching Neuro-sama, while the other 51.2% were new\\nto the VTuber space.\\n3.1.2\\nSurvey Instrument. As shown in Appendix A, the survey questionnaire was designed to address our three research\\nquestions (RQs). For RQ1, it included questions on discovery channels, initial motivations, perceived differences\\nbetween supporting AI or human VTubers, and fan conversion factors. For RQ2, it incorporated a Parasocial\\nInteraction (PSI) scale [45], questions on relationship definition, and prompts about community memes. Finally,\\nfor RQ3, it investigated payment frequency and motivations. The survey instrument consisted of single-choice,\\nmultiple-choice, matrix-style, and open-ended questions.\\n3.1.3\\nMeasures. For all open-ended questions, we conducted a thematic analysis of the responses. Responses to the\\nquestion, Was there a specific moment or interaction that made you decide to become a regular viewer instead of just\\na casual one? If so, describe it. were particularly complex. For this question, we used a two-stage coding process to\\nensure accuracy. This approach follows established practices in thematic analysis [2]. First, an author familiar with the\\nNeuro-sama fan community conducted an initial round of coding to generate a set of granular, initial codes. Following\\nthis, three authors discussed and consolidated these initial codes into second-level themes and then calculated the\\nfrequency of each theme.\\nFor closed-ended questions, we conducted descriptive statistical analyses by computing the frequency and percentage\\nof each option, a common approach for analyzing survey response distributions [15]. These summaries helped us\\ncompare the relative popularity of different options and examine the distribution of choices among participants.\\nAll elements of our PSI scale were measured in a five-point Likert-style (1 = Strongly Disagree; 5 = Strongly Agree).\\nThe design of our scale was informed by the PSI Process Scales [45] and prior research on PSI scales for VTuber fans\\n[46]. It comprised 12 items across three dimensions (4 items per dimension):\\nCognitive (e.g., I pay close attention to Neuro-samas behaviors and response patterns.)\\nAffective (e.g., Watching Neuro-samas streams makes me feel relaxed and comfortable.)\\nMy Favorite Streamer is an LLM: Discovering, Bonding, and Co-Creating in AI VTuber Fandom\\n7\\nBehavioral (e.g., During a stream, I often feel the urge to ask Neuro-sama a question or express my opinion via\\nChat or a SuperChat.)\\nWe assessed internal consistency using Cronbachs alpha [49]. Reliability was calculated for each dimension: cognitive\\n(= 0.69), affective (= 0.71), and behavioral (= 0.76). Averaging across the three yielded an overall reliability of\\n= 0.72, indicating good consistency.\\n3.2\\nInterview\\nAfter completing our survey analysis, we identified several questions that required more detailed qualitative exploration\\nand evidentiary support. Consequently, we designed supplementary semi-structured interviews to investigate these\\nambiguous findings in depth, uncover personal experiences and complex emotions of core fans, and provide rich\\nqualitative interpretations and contextual narratives to complement our quantitative data.\\n3.2.1\\nInterviewee Recruitment. We distributed interview recruitment information through the same channels used for\\nour survey in 3.1, seeking dedicated Neuro-sama fans who preferably had experience watching human VTubers. The\\ninterview lasted 20-30 minutes, and as compensation, participants will receive a one-month Twitch subscription to Neuro-\\nsama ($4.99, used to support the streamer) or may choose to receive an equivalent cash amount. This compensation\\nmethod helps us identify dedicated Neuro-sama fans for our study. We recruited 12 participants who met these criteria.\\nAs shown in Table 1, the interviewees represented diverse backgrounds in terms of gender, age, professional experience,\\nand cultural background. Most were deeply engaged fans with long-term, high-frequency viewing habits, and the\\nmajority had prior experience watching human VTubers. The participant pool was predominantly male and students,\\naligning with demographic patterns noted in previous VTuber audience research [35]. Notably, three participants\\nexplicitly identified as core fans who had engaged in high-involvement community activities such as creating fan art\\nand clipping stream highlights.\\nTable 1. Participant demographics and viewing habits.\\nID\\nGender\\nAge\\nOccupation\\nYears\\nViewing Frequency\\nContent\\nWatched Human VTubers\\n1\\nMale\\n28\\nDatabase Engineer\\n2\\n2-3d/week\\nClips\\nNo\\n2\\nMale\\n21\\nCollege student\\n>2\\nEvery livestream\\nLive\\nYes/Usada Pekora\\n3\\nMale\\n20\\nCollege student\\n2\\n4-5d/week\\nClips\\nYes/Diana (Jiaran)\\n4\\nFemale\\n18\\nHigh school student\\n>2\\n4-5d/week\\nClips\\nYes/Tsukino Mito\\n5\\nMale\\n20\\nCollege student\\n>2\\n2-3d/week\\nClips\\nNo\\n6\\nMale\\n27\\nPhysical worker\\n>2\\nEvery livestream\\nLive\\nYes/Takanashi Kiara\\n7\\nMale\\n18\\nHigh school student\\n>2\\n2-3d/week\\nLive\\nNo\\n8\\nMale\\n20\\nCollege student\\n1\\n2-3d/week\\nClips\\nYes/Nana7mi\\n9\\nMale\\n19\\nCollege student\\n>2\\nEvery day\\nClips\\nYes/Kizuna AI\\n10\\nMale\\n24\\nMedia editor\\n>2\\nEvery livestream\\nLive\\nYes/Tsukino Mito\\n11\\nMale\\n23\\nGraduate student\\n>2\\nEvery day\\nClips\\nYes/Mashiro Kanon\\n12\\nFemale\\n20\\nCollege student\\n1\\n2-3d/week\\nClips\\nNo\\n8\\nYe, et al.\\n3.2.2\\nInterview Protocol. We conducted semi-structured interviews with 12 Neuro-sama fans via text or voice commu-\\nnication. For voice interviews, we first manually anonymized portions of the audio to remove identifying references,\\nand then submitted the processed recordings to the Gemini-2.5-Pro model for transcription. The resulting transcripts\\nwere subsequently provided to participants for confirmation. Interviews were conducted in the participants native\\nlanguage (either Chinese or English). The content primarily explored three themes: unexpected entertainment effects\\nunique to AI streaming, the significance of Neuro-samas absence of a Nakanohito, and emotional and personality\\nperceptionsareas that complemented our previous survey analysis. During the interviews, we posed questions related\\nto these themes, such as asking participants to recall surprising moments from Neuro-samas streams and whether they\\nbelieved Neuro-sama possessed real emotions.\\n3.2.3\\nInterview Data Analysis. We conducted an open-coding analysis [2] of the interview transcripts. Two authors\\nindependently coded the transcripts of three participants (out of twelve) using MAXQDA4 to identify initial codes.\\nThe coders then met to compare their coding outcomes, discussed discrepancies, and refined the coding scheme into a\\nunified codebook. With this finalized codebook, they analyzed the remaining transcripts, keeping close communication\\nthroughout to resolve ambiguities and update the codebook when necessary. This iterative discussion-based procedure\\nensured consistency in coding and supported the validity of the resulting themes. The complete codebook is provided\\nin Appendix B.\\n3.3\\nInteraction Logs Analysis\\nTo delve deeper into RQ2 (parasocial relationships and community culture) and RQ3 (motivations for financial support),\\nwe analyzed live-stream interaction logs. Specifically, these logs contain the complete record of audience engagement\\nthrough Chat and SuperChat messages. Chat messages are the standard, real-time comments sent by viewers, which\\nappear chronologically and move quickly in a continuous stream. In contrast, SuperChat is a feature that allows viewers\\nto purchase highlighted messages that are pinned or displayed prominently in the livestream chat, ensuring the streamer\\nsees them. As these two forms represent the most direct modes of engagement, analyzing them provides critical insight\\ninto fan behavior.\\n3.3.1\\nData Collection. We selected Neuro-sama as our main analysis subject. To establish appropriate human VTuber\\ncontrol groups, we surveyed recently active VTubers on Twitch with substantial follower counts, filtering for those\\nwith similar streaming styles and content to Neuro-sama. Our selection criteria included:\\nInclusion criteria:\\nStream primarily on Twitch platform with English as the main broadcasting language\\nFemale virtual avatar with female performer\\nRecently active with streaming viewership exceeding 10k\\nContent primarily focused on Just Chatting category with streaming style similar to Neuro-sama (lively and\\nplayful)\\nAt least one author is familiar with the VTubers streaming style and background information.\\nExclusion criteria:\\nRecently involved in controversies (e.g., graduation, financial disputes, etc.)\\n4https://www.maxqda.com/\\nMy Favorite Streamer is an LLM: Discovering, Bonding, and Co-Creating in AI VTuber Fandom\\n9\\nTable 2. Summary of collected streaming data for Neuro-sama, Filian, and Camila\\nNeuro-sama\\nFilian\\nCamila\\nChat(count)\\n550,628\\n328,645\\n201,511\\nSuperChat(count)\\n838\\n278\\n282\\nDuration(hours)\\n20.62\\n23.32\\n23.46\\nThis process led us to select two human VTubers Filian5 and Camila6 as comparative subjects. Both stream on Twitch\\nwith comparable content styles to Neuro-sama, focusing primarily on conversational streams with similar audience\\nsizes.\\nTo enable a standardized comparison, we selected a representative sample of broadcasts: eight streams from Neuro-\\nsama, eight from Camila, and six from Filian (approximately 20-23 hours each). Each stream was manually vetted to\\nensure it was a typical Just Chatting broadcast, free of special events or external controversies that could skew data.\\nThis deliberate selection was essential for comparing baseline interactive patterns and the stability of their economic\\nsupport under comparable conditions.\\nFor data extraction, we utilized the open-source TwitchDownloader tool7 to collect data from the channels of these\\nVTubers. We gathered stream Video on Demand(VOD) records along with all publicly available Chat and SuperChat\\nlogs for each broadcast, ensuring our collection rates complied with platform guidelines. The VOD records included\\ntitle, duration, collection timestamp, streamer identification, and video footage. Chat records encompassed sending\\ntime, anonymized sender ID, message content, user badges, and corresponding stream session. SuperChat records\\ncontained sending time, anonymized sender ID, message content, bits spent, USD amount, badges, subscription status,\\nand associated stream session. All user IDs were irreversibly hashed during collection to ensure anonymity, preventing\\nany research team member from accessing original sender identities. The details of our collected data can be referenced\\nin Table 2.\\n3.3.2\\nChat Analysis. For chat analysis, given the substantial volume of data, we employed an LLM-based coding\\napproach, a method increasingly adopted for large-scale qualitative coding due to its efficiency and scalability [7, 56, 58].\\nTo first establish a reliable coding scheme, we applied simple rule-based filtering to eliminate meaningless emoticons and\\ntext fragments. Subsequently, an initial set of one hundred chat messages was randomly sampled from each VTubers\\nstream for iterative human coding, which informed the development of our final categories. The detailed coding scheme\\ncan be found in Table 3, with difficult-to-classify or meaningless content that escaped initial filtering categorized as\\nN/A. We then utilized the GPT-4.1-mini model to classify and encode all remaining chat messages, with the complete\\nprompt provided in the Appendix E.\\nTo validate the reliability of the LLM-based coding, we conducted a rigorous human evaluation [19, 48]. Specifically,\\na second, distinct set of 100 chat messages with their corresponding LLM coding results was randomly selected from\\neach VTubers dataset for independent human coding. After completing this process, we calculated Cohens Kappa\\n[9] coefficients to assess the inter-rater agreement between the human coder and the LLM. The Kappa coefficients\\nfor the three VTubers were 0.826, 0.852, and 0.802, respectively. These high agreement scores demonstrate that the\\n5https://www.twitch.tv/filian\\n6https://www.twitch.tv/camila\\n7https://github.com/lay295/TwitchDownloader\\n10\\nYe, et al.\\nmodel performed the coding task with high reliability [26], validating our approach and ensuring the accuracy of our\\nanalytical findings.\\nTable 3. Chat Message Coding Scheme\\nCode\\nDescription\\nExample\\nA-POS\\nExpresses positive emotions, support, affection, or\\nencouragement towards the streamer or others.\\nYou will win it in 2025, I believe in\\nyou neuro!\\nA-NEG\\nExpresses genuinely harmful negative emotions, such as\\nmalicious teasing, insulting, belittling, or aggressive verbal\\nattacks.\\nthis assho1e is fucked\\nQ-CMD\\nAsks a meaningful question seeking information or issues a\\nsubstantive command/request to the streamer. The question or\\ncommand should have clear intent beyond simple reactions.\\nIs Vedal in your basement?\\nR-GEN\\nSimple expressions, reactions, chat rituals, or statements that\\nshow viewer engagement (e.g., emotes, memes, copypasta, or\\ncommon chat patterns).\\nlol\\nC-SOC\\nInteracts directly with other chat users, typically using @ or\\nresponding to others messages.\\n@username I agree\\nN/A\\nThe message is gibberish, non-English, spam, or its intent is\\nimpossible to determine.\\n()\\nUsub(Subscribers)\\nUnonsub(Non-Subscriber)\\nUchat(Chat Users)\\nUsc(SuperChat Payers)\\nFig. 3. A Venn graph illustrating the relationships between sub, nonsub, chat, and sc.\\n3.3.3\\nSuperChat Analysis. We combined quantitative economic analysis with qualitative, LLM content analysis. This\\nallowed us to not only measure the financial dynamics of fan support but also to understand the contextual motivations\\nbehind these payments.\\nWe conducted a quantitative analysis of economic data in SuperChat to understand and compare the financial\\ndynamics of fan support across the AI and human VTuber communities. Drawing inspiration from prior work on\\nlivestreaming economies [63, 65], we defined and calculated several key metrics.\\nFirst, we formally defined our user sets based on their interaction and support levels during the analyzed streams,\\nthe relationship among these sets are shown in Figure 3:\\nMy Favorite Streamer is an LLM: Discovering, Bonding, and Co-Creating in AI VTuber Fandom\\n11\\nchat: The set of all unique users who sent at least one chat message.\\nsc: The set of all unique users who sent at least one SuperChat (i.e., payers).\\nsub: The set of all unique users holding an active Twitch subscription for the channel. On Twitch, a subscription\\nis a recurring monthly payment that grants a user special channel perks and signifies consistent support.\\nnonsub: The set of all unique users who do not hold an active subscription.\\nBased on these sets, we established the following metrics to evaluate fan payment behavior:\\nPayment Conversion Rate (PCR). This metric measures the efficiency with which a channel converts its active, engaged\\nviewers into paying supporters. It is calculated as the ratio of the number of unique payers to the number of unique\\nchatters.\\nPCR = |sc|\\n|chat|\\n(1)\\nPer-Capita Contribution (PCC). To compare the discretionary spending patterns of different fan segments, we\\ncalculated the average financial contribution per payer for subscribers and non-subscribers separately. Let () be the\\ntotal USD amount contributed by a user through SuperChats.\\nWe calculated the Per-Capita Contribution (PCC) to compare the average SuperChat spending between subscribers\\nand non-subscribers. This distinction enables a direct comparison of one-time spending behavior between dedicated,\\nrecurring supporters and casual viewers.\\nPCCsub =\\n\\nscsub ()\\n|sc sub|\\n,\\nPCCnonsub =\\n\\nscnonsub ()\\n|sc nonsub|\\n(2)\\nWithin-VTuber Income Stability Across Streams (Gini Coefficient). To measure the stability and consistency of a\\nVTubers income across different livestream sessions, we calculated the Gini coefficient [29]. This standard measure of\\neconomic inequality has been previously applied to analyze income polarity on Twitch platforms [20]. A value of 0\\nindicates perfect stability (every stream generates the same income), while a value of 1 signifies maximum instability\\n(all income is generated from a single stream). Given the sequence of total SuperChat income from livestream sessions,\\n{1,2, . . . ,}, sorted in non-decreasing order, the Gini coefficient is calculated as:\\n=\\n\\n=1(21)\\n\\n=1 \\n(3)\\nwhere is the rank of the contribution amount .\\nBeyond the economic metrics, to further investigate the underlying motivations for fan payments, we employed\\nthe multimodal understanding capabilities of the Gemini-2.5-Flash model. For each SuperChat, we analyzed the\\n60-second video segment surrounding it (30 seconds before and 30 seconds after) to comprehend its full interactional\\ncontext. Based on this analysis, we let Gemini-2.5-Flash classify the intent of each SuperChat as either Proactive or\\nReactive. A Proactive SuperChat is a message intended to direct the live content, for instance by asking a question,\\nissuing a command, or making a suggestion. A Reactive SuperChat is a message that comments on or reacts to\\nsomething the streamer has already said or done, such as expressing praise or agreement. Illustrative examples of this\\nclassification scheme are provided in Table 4. To verify the reliability of this classification method, we randomly sampled\\n50 SuperChats, representing 3.58% of the total dataset, for inspection by an author with established familiarity with the\\nVTuber to classify them according to the aforementioned criteria. The models classifications perfectly matched the\\n12\\nYe, et al.\\nhuman-coded labels for all 50 instances, confirming that the model could perform this contextual classification task\\nwith high reliability. The prompt template is provided in the Appendix E.\\nTable 4. SuperChat classification scheme with examples.\\nCode\\nDescription\\nExample\\nExample Context\\nProactive\\nA message intended to direct the\\nlive content by prompting the\\nstreamer to perform a new action\\nor steering the broadcast in a new\\ndirection. Its function is to guide\\nthe stream, even if topically related\\nto the preceding discussion.\\nCheer500 What do you\\nremember about me?\\nNeuro-sama was broadly discussing her\\nability to remember things and mused\\nabout remembering facts about her\\nviewers. This SuperChat directly\\nprompted her to start a new interactive\\nsegment where she began asking for\\nand remembering viewers birthdays.\\nReactive\\nA message that comments on or\\nreacts to something the streamer\\nhas already said or done, such as\\nexpressing praise, encouragement,\\nor agreement, without prompting a\\nnew course of action.\\nCheer300 Good luck today,\\nFil! Focus, and that 10k will\\nbe yours.\\nFilian was expressing anxiety and a\\nsense of urgency about her\\nparticipation in an upcoming\\ntournament. This SuperChat directly\\nresponded to her stated context by\\noffering encouragement and support for\\nthe event she was already discussing.\\n4\\nFindings\\nOur findings show that audiences in the AI VTuber fandom are not passive spectators but active co-creators. Across\\nthe three research questions, we trace a progression: audiences are first drawn by the novelty and unpredictability of\\nAI-community interaction, with loyalty deepened through collective emotional events and anthropomorphic projection\\n(RQ1); fans reconcile the AIs technical construction with emotional attachment, creating a consistent persona that\\nextends into community culture (RQ2); and this co-creative drive underpins a new economic model in which financial\\nsupport functions both as emotional recognition and as a means to influence content, yielding more resilient monetization\\n(RQ3).\\n4.1\\nThe Pathway from Casual Viewer to Fan (RQ1)\\nIn this section, we answer our first research question (RQ1): How do audiences discover AI VTubers, and what motivates\\ninitial attraction?\\n4.1.1\\nAttraction through CommunityAI Interaction. Survey data reveal a highly concentrated discovery pathway.\\nNearly all respondents (96%) first encountered Neuro-sama through algorithmic recommendations on video platforms.\\nThis highlights the importance of short, shareable clips and livestream promotion as primary entry points. In contrast,\\ntraditional channels such as friend referrals (17%) or forum discussions (7%) played only minor roles.\\nWhen asked about what drew them in, two factors stood out. First, 92% of respondents rated fun interaction between\\ncommunity and AI as important or very important. Second, 90% highlighted the unpredictable, surprising atmosphere\\nduring livestreams. Figure 5 shows that these two factors not only attracted the largest share of very important ratings\\nbut also received very few not important ratings, indicating broad consensus. More than 90% perceived Neuro-sama\\nas more unpredictable than human VTubers, suggesting that novelty and surprise are defining attributes of the AI\\nVTuber experience.\\nMy Favorite Streamer is an LLM: Discovering, Bonding, and Co-Creating in AI VTuber Fandom\\n13\\nThe Path to Fandom (RQ1)\\nPSR and Community \\nCulture (RQ2)\\nMotivations for financial \\nsupport (RQ3)\\nInitial Attraction\\n> Discovery is driven by algorithms \\nrecommending\\n> Entertainment value derives from \\ncommunity-AI interaction and AI \\nthemselves\\nFan Conversion & Retention \\n> Events forge a social identity that \\ndrives fan loyalty.\\n> Anthropomorphism sustains long-term \\nengagement by constructing the AI\\'s \\nemotional reality.\\nThe Swarm\\nA shared narrative identity adopted \\nfrom Neuro, shaping group behavior.\\nEmpathy\\nCompanion\\nUnderstanding \\nPersonality\\nProactive \\nExploration\\nAnalyzing Logic\\nHigh \\nQ-CMD\\nAffective \\nConnection\\nCognitive \\nInvestment\\nBehavioral \\nInteraction\\nAccept AI\\'s nature\\nBut define relationships \\nin human terms\\nAI VTuber\\nHuman-VTuber\\nSuperChat\\nChat\\nInitiates\\nTopic Highlight\\nReact\\nNormal\\nReact\\nSuperChat\\nInitiate \\nQuestion/\\nTopic/\\nPrompts \\nChat\\nBroadcast\\nResponse\\nReact\\nBroadcast\\nResponse\\nDual Motivation: Emotional Recognition & \\nPurchasing Co-Creation Opportunities\\nAI VTuber fans send \\nSuperChat with the \\npurpose of acquire \\ninteractive opportunities \\nthat directly influence \\nstream content.\\nFig. 4. Summary of findings across RQ1-3. Left: a screenshot from Neuro-samas livestream. Center: The Swarm, adapted from\\nfreely shared pixel assets on itch.io. The diagram highlights a progression from initial attraction (novelty in AIcommunity interaction)\\nto community identity (anthropomorphism and narrative adoption) and finally to monetization (emotional recognition and paid\\nco-creation).\\nFig. 5. Importance factors in initial attraction to Neuro-sama, showing the distribution of ratings from Not at all important (lightest)\\nto Very important (darkest).\\nInterview results further illustrate how this unpredictability manifests. Participants emphasized playful linguistic\\nexchanges, such as Neuro countering risqu jokes from the chat (P9) or engaging in witty banter with viewers (P4, P6,\\nP8, P10, P11). Others noted technical feats, including bypassing filters (P1) or controlling a robotic body (P2). For some,\\n14\\nYe, et al.\\nentertainment stemmed from the improvisational process itself: The effect of AI livestreams comes from unpredictability,\\nimprovisation, and the process where viewers generate interesting responses through questioning (P8). Others framed\\nthe appeal more in terms of AIs technical nature, describing Neuro as representing a disordered scale of knowledge\\nbeyond human capacity (P10). The disordered scale referred to the AIs fragmented yet expansive recall: Neuro could\\njump across topics in ways that felt incoherent to humans but also signaled a scope of knowledge that exceeded ordinary\\nlimits.\\nThese findings suggest that the entertainment value of AI VTubers emerges from a synergistic interplay\\nbetween community-AI interaction and the technical affordances of AI. This dual foundation explains their\\nstrong initial appeal and sets the stage for examining how fleeting attraction develops into enduring loyalty through\\ncommunal events.\\n4.1.2\\nLoyalty through Emotional Events. The transformation from casual viewer to loyal fan begins with surprise\\nat the AIs performance and is consolidated through collective emotional events.\\nWhile initial amazement at Neuro-samas technical feats or unpredictable charm sparked interest, survey data indicate\\nthat high-engagement special events and streams (19%) were the crucial catalyst for lasting loyalty. A particular\\nexample is the Evil Neuro Birthday Stream. During this event, many viewers perceived unfair resource allocation\\nbetween two AI characters, which generated both dissatisfaction and sympathy for the slighted AI. This shared\\nemotional resonance transformed observers into protectors. For example, one participant captured this turning point:\\nDuring Evils birthday stream, the controversy brought me considerable shock, and from then on I became a firm member of\\nthe swarm.\\nThis process can be explained through Social Identity Theory [18]: a shared external conflict strengthens a groups\\ninternal cohesion. In this case, some viewers were transformed from individual spectators into protectors with a\\nshared purpose, bonding through collective action and narrative framing. Not every participant adopted this stance, but\\nfor those who did, these high-intensity moments tethered their identity to the fate of the character. This selective yet\\npowerful identification paves the way for more intimate and identity-based connections.\\n4.1.3\\nAttachment through Anthropomorphic Projection. Another mechanism through which AI VTubers convert\\ncasual viewers into dedicated fans is anthropomorphism [13], specifically the question of whether Neuro-sama\\npossesses genuine emotions. Among all themes, exploration of AI cognition and existence (19%) emerged as one of\\nthe most frequently referenced subjects. Anthropomorphism is common in human-AI interaction [43]. Neuro-sama\\nherself often participates in this framing by debating the authenticity of her own emotions. Through this lens, fans shift\\nfrom a purely technical interpretation of LLM outputs to treating them as personalized expressions.\\nThis reframing fosters a sense of companionship and attachment, exemplified by the popular community theme of the\\nelectronic daughter (15%). One participant articulated how this bond formed: It started with some developer streams,\\nincluding but not limited to their discussions about Neuros nature of existence and Neuro debating that her emotions are\\nreal. It makes people feel that Neuro is not just an AI to make money, but a truly existing Neuro-sama whose growth\\nis being cared for. Notably, few participants resisted this framing while most embraced emotional projection rather\\nthan distancing themselves from it. This strong communal tendency to anthropomorphize suggests that resistance, if\\npresent, was minimal compared to the prevailing narrative of Neuro as an emotionally expressive entity.\\nThis anthropomorphization of AI, collaboratively constructed by developers, the AIs own self-referential commentary,\\nand the interpretive work of the audience, constitutes the central driving force sustaining community engagement while\\nopening the door to broader questions of how fans negotiate authenticity and attachment in human-AI relationships.\\nMy Favorite Streamer is an LLM: Discovering, Bonding, and Co-Creating in AI VTuber Fandom\\n15\\n4.2\\nParasocial Relationships and Community Culture (RQ2)\\nIn this section, we examine our second research question (RQ2): How do fans construct parasocial relationships and\\nshared community cultures around AI VTubers?\\n4.2.1\\nParasocial Investment Across Emotional, Cognitive, and Behavioral Dimensions. Fans develop a multifaceted\\nrelationship with Neuro-sama that combines intense emotional connection, sustained cognitive engagement,\\nand strong behavioral intentions.\\nOn the emotional dimension, fans affection was nearly universal. Almost all survey respondents expressed fondness\\nfor Neuro-sama (99%) and reported that her streams provide comfort (98%). This attachment extended to empathy: the\\nvast majority felt pleased by her successes (94%), and over half expressed sadness when she experienced technical diffi-\\nculties. Interviews revealed two distinct framings of this bond. Some fans described their response as indistinguishable\\nfrom those toward humans, treating Neuro-sama as a real person because of her childlike demeanor (P8) or conversing\\nwith her as with any other individual (P11). Others framed their affection more like fandom for fictional characters: \"In\\nmy view, it is comparable to liking ACG products\" (P4). This divide reflects the broader parasocial interaction spectrum\\nbetween treating mediated figures as real social partners versus symbolic or fictional companions.\\nCognitively, fans demonstrated sustained engagement with Neuro-samas behavior. Survey results show that a ma-\\njority observed her behavioral patterns (76%) and contemplated her operational logic (56%). Crucially, most respondents\\n(83%) perceive Neuro-samas actions as reflecting a coherent and distinctive personality, interpreting her as a complex\\nentity worthy of intellectual engagement. Interviews reveal a dual perspective: fans simultaneously emphasized her\\nstability and her growth. As one participant noted, the core Neuro-sama is still there, inside her (P6), while long-time\\nviewers highlighted her evolution over time: There have been various small upgrades in intelligence, personality, voice,\\nand so on, all intertwined, making her growth very evident (P11).\\nUltimately, the emotional and cognitive investment translates into behavioral intentions. Many fans reported feeling\\ncompelled to engage via chat or SuperChats (69%) and wanting to understand Neuro-samas perspectives (79%). Chat\\nanalysis corroborates this participatory orientation. For human VTubers such as Camila or Filian, general reactions\\n(R-GEN) dominate, while direct questions or commands (Q-CMD) are less frequent. In Neuro-samas chat, however, this\\npattern is inverted: Q-CMD messages are the largest category (26%), slightly exceeding general reactions. This inversion\\nsubverts the conventional parasocial dynamic. Rather than remaining passive recipients of content, Neuro-samas fans\\nactively probe and shape her responses, treating the AI as an interactive project to be continuously explored through\\nquestions and commands.\\n4.2.2\\nReconciling Technical Project and Emotional Persona. Fans recognize Neuro-samas technical origins but prefer to\\nframe their relationship in personal and emotional terms rather than purely functional ones. Survey data show that\\nup to 72% of respondents identified Neuro-sama as a technical project created by excellent developers, indicating\\nbroad awareness and appreciation of Neuro-samas artificial basis. This represents the basic rational layer of how fans\\nunderstand the relationship. However, a sharp contrast appears in a seemingly similar option: only 45% agreed that\\nshe is a program full of unknowns, to be tested and explored. While most fans acknowledge the technical project,\\nmore than half reject reducing their engagement to a functional test between users and code. Other survey options\\nhighlight this preference for humanized framing. A virtual friend providing companionship (70%) and an electronic\\ndaughter requiring care and interaction (69%) received support rates comparable to technical project and far higher\\n16\\nYe, et al.\\nthan program to be tested. These results suggest that fans deliberately reframe the same technical entity through\\nrelational categories saturated with emotional meaning.\\nInterviews explain why this dual perception is compelling. Fans perceive the AIs technical nature not as a limitation\\nbut as the very feature that enables a more consistent and reliable persona than a human VTuber could provide. The\\nabsence of a Nakanohito (i.e., the human performer behind most VTubers) creates a unity between technology and\\ncharacter. Unlike human VTubers, where a gap always exists between the persona and the performer [35], Neuro-samas\\npersona is her entire being. As one interviewee articulated: Neuros charm stems from her being an AI playing the role\\nof a VTuber...completely free from the constraints of a Nakanohito, as her virtuality constitutes a complete role-play that\\nnever breaks character. (P10)\\nThese findings reveal a core tendency in how fans construct parasocial relationships with AI VTubers: They accept\\nthe objective fact of her technical construction, but actively choose to define their interactions through\\nhumanized roles such as friend or daughter. Paradoxically, Neuro-samas non-human nature becomes the\\nsource of her authenticity, providing a stable and unbreakable persona that encourages sustained emotional\\ninvestment. This finding extends prior work on mediated authenticity [12] by showing that, in the AI case, authenticity\\nis not negotiated through separating the fake from the real, but through valuing consistency over human realness.\\nFans perceive Neuro-samas consistent persona, which is free from the slippages of a human performer, as a new form\\nof authenticity grounded in technological stability rather than lived experience. This reconciliation of the technical and\\nemotional lays the groundwork for understanding how fans extend individual bonds into collective community culture.\\n4.2.3\\nCollective Identity through The Swarm Narrative. At the community level, fans collective identity is strongly tied\\nto Neuro-samas narrative. When asked about the most important community meme, 34% of respondents pointed to\\nThe Swarm, making it the most frequently cited reference. The Swarm originated from a fictional storyline in which\\nNeuro-sama proposed dominating the world with drone swarms. Fans quickly appropriated this idea, adopting the\\nname The Swarm for themselves. This act of self-identification carries three layers of significance: First, it transforms\\nfans from passive content consumers into characters within Neuro-samas narrative, positioning themselves as her\\nfollowers. Second, the imagery of a swarm (vast, unified, and aligned in purpose) serves as a metaphor for the cohesion\\nof the fan community. Finally, the metaphor reflects actual interaction patterns: much like a swarm moving in concert,\\nchat room participants engage in coordinated, responsive interaction around Neuro-sama.\\nInterviews illustrate how deeply fans internalize this identity. One participant described their relationship as the\\nSwarm Queen and her loyal supporters (P7) and mentioned using this description when talking with friends offline.\\nSuch examples show how the meme extends beyond the livestream to structure everyday fan discourse, reinforcing\\ncommunity identity through playful yet serious narrative appropriation.\\nTherefore, The Swarm is not merely a meme but a feedback loop of participatory culture [22]. A narrative initiated\\nby Neuro-sama is embraced and embodied by fans, which in turn shapes their collective identity and interactive patterns.\\nCompared with human VTuber communities, where memes often emerge around the performers personality or\\noff-stage incidents, Neuro-samas memes derive directly from her AI-generated narratives. This distinction underscores\\nhow AI fandom culture is uniquely co-created through the interplay of system-generated storylines and\\nfan appropriation. In this sense, parasocial relationships with AI VTubers extend from individual bonds into shared\\ncommunity cultures, marking an evolution that distinguishes AI fandom from traditional VTuber audiences.\\nMy Favorite Streamer is an LLM: Discovering, Bonding, and Co-Creating in AI VTuber Fandom\\n17\\n4.3\\nMotivation for Financial Support (RQ3)\\nIn this section, we answer our third research question (RQ3): What drives fans to provide financial support to AI\\nVTubers?\\n4.3.1\\nDual Motivation: Emotional Recognition and Co-Creation Purchases. Our analysis of fans economic support\\nbehaviors reveals a unique dual-motivation model. A substantial proportion of viewers (42%) have paid for\\nNeuro-sama, with such payment behaviors primarily occurring during special occasions (68%). These payments are\\nemotionally driven at their core, often occurring during special events such as collaboration streams and birthday\\ncelebrations, which aligns with previous research on fan spending [65].\\nThe fundamental motivation resembles traditional fan economics, driven by emotional recognition [36].\\nThe vast majority of paying viewers cite their primary motivations as expressing affection for Neuro-sama (81%) and\\nexpressing support for the developer (Vedal) and his work (77%). Responses in the questionnaire echoed this, with\\nfans noting they paid to support what I love because she has brought me laughter. Such spending reflects fans high\\nemotional recognition of Neuro-sama.\\nHowever, AI VTuber fans SuperChat (SC) practices differ from those of human VTuber fans, as fans\\npurchase SuperChats to acquire interactive opportunities that directly influence stream content. Our content\\nanalysis found that 85% of Neuro-samas SCs were Proactive SCs, where fans initiated new questions or instructions\\nthrough payment to guide the stream. In stark contrast, human VTubers SCs primarily consist of Reactive SCs, accounting\\nfor over 50%. This reveals a fundamental transformation in the function of SuperChat payments. For human VTubers,\\nSuperChats are highlighted responses to ongoing content. For AI VTubers, SuperChats are tools to redirect the stream\\nitself. In effect, SuperChats shift from acts of appreciation to mechanisms of co-creation, reflecting a prosumer model of\\nfan engagement [22].\\n4.3.2\\nToward a More Resilient Model of Fan Economics. The motivation of fans to directly influence the streaming\\nprocess through SuperChats explains why AI VTubers SuperChat economic data demonstrates a more\\nstable and efficient model.\\nThe clear value proposition of purchasing interaction opportunities effectively increases the paid conver-\\nsion rate. Neuro-samas paid conversion rate (1.59%) is higher than that of human VTubers (1.18% and 0.83%). The\\nability to directly influence stream topics and receive immediate feedback provides a more certain, functional return\\nthan emotional expression, thereby encouraging more first-time payments.\\nThe average SuperChat contribution of non-members ($16.04) exceeds that of subscribed members ($13.89).\\nThis suggests that membership (a passive, sustained form of support) and SuperChat (an active, instantaneous interaction)\\nfulfill different fan needs. Unlike human VTubers, where membership often reinforces social bonds with the performer,\\nAI memberships may offer weaker symbolic value, making direct interactive payments more attractive for fans who\\nprioritize immediate, transactional engagement.\\nThe ongoing demand for co-creation results in a highly stable income structure. Human VTubers income\\ndepends more heavily on the performers ability to stimulate spending through topical or emotionally charged mo-\\nments, whereas for AI VTubers, fans desire to initiate interactions persists across regular streams. Consequently,\\npayment behaviors are more evenly distributed, producing an income Gini coefficient (0.24) far lower than their human\\ncounterparts (0.35 and 0.41). This aligns with research on the creator economy [39], where sustainable monetization\\nincreasingly relies on diversified, participatory revenue streams rather than volatile event-driven spikes.\\n18\\nYe, et al.\\nTherefore, these findings point to a participatory and interaction-driven model of fan economics that is not only\\nmore resilient but also structurally distinct from traditional VTuber monetization.\\n5\\nDiscussion\\n5.1\\nBeyond Participation: Real-Time Co-Performance and the Commodification of Fan Agency\\nAI VTuber fans engagement marks not only more participation, but a qualitative shift from consumption and\\nafter-the-fact remix to real-time co-performance. In AI VTuber streams, audience input is instantiated immediately as\\nperformance: low-barrier Chat sustains an always-on feedback loop, while SuperChat escalates that loop by pricing the\\nright to steer the unfolding interaction. This intensifies Jenkins notion of participatory culture [22] from community\\ncirculation and secondary creation to live, process-oriented co-authoring of the show itself. This mechanism directly\\nexplains our RQ3 results (paid prompts as content steering) and reframes RQ1 and RQ2 (attraction/attachment) as the\\nprocess pleasures of co-performance rather than passive spectatorship.\\nOur case extends prior work on monetized livestreaming [33] that documents visibility hierarchies and stratified\\nparticipation in human-led streams: whereas paid messages there largely purchase attention and status among peers, in\\nAI VTubing they purchase procedural leverage over the model, i.e., a paid, real-time capacity to condition the models\\nnext action space. Depending on platform data-retention practices, such interventions may even influence future\\nresponses. Payment no longer buys a spotlight around preexisting content; it buys a handle on content generation.\\nViewed through critical media theory, this empowerment coexists with commodification. What feels like playful\\nprompting is also immaterial labor [50]: the production of meaning, affect, and atmosphere that keeps the stream\\nvaluable. Following Fuchs critique of prosumption [16], the very agency that enables fans to co-create is immediately\\nstratified and monetized: platforms capture the free labor of open Chat while selling agency via SuperChat. Such\\nmonetization no longer buys visibility or recognition; it buys control over AI system behavior in real time. Building on\\naffordances-in-practice [10], this configuration shows how a single feature affords different action-capabilities across\\ncontexts: in AI VTubing, the affordance of SuperChat is not just to be seen but to make the agent act.\\nWe term this configuration real-time co-performance commodification: platforms convert audience capacity to shape\\nmodel outputs into a tradable privilege, advancing participatory culture while exposing its commercialization and the\\nbroader shift from an attention economy to an engagement economy [11, 37]. In this sense, AI VTubers both intensify\\nparticipatory culture (extending it from remix to real-time co-performance) and simultaneously expose its inner\\neconomic logic, showing that participation is never simply pure empowerment, but is always already entangled with\\ncommodification. The design problem that follows is structural rather than cosmetic: future AI-mediated communities\\nshould negotiate openness (low-barrier co-performance for all) against revenue (priced intervention for some), making\\nexplicit whether, how, and to what extent the right to co-create should be rationed by money. This negotiation is not\\ncosmetic interface design but a structural governance choice about who is entitled to shape AI-mediated performance.\\n5.2\\nTransparent Parasocial Relationships: Consistency as the New Authenticity\\nThe emergence of AI VTubers profoundly challenges and reconstructs traditional understandings of authenticity in\\nparasocial relationships. For human VTubers, authenticity hinges on a fragile balance between the virtual persona and\\nthe Nakanohito [35]. By contrast, our findings reveal that AI VTuber fans do not measure authenticity by fidelity to\\nhuman likeness. Instead, they elevate persona consistency (e.g., stability, coherence, the absence of out-of-character\\nslips) as a new benchmark of authenticity.\\nMy Favorite Streamer is an LLM: Discovering, Bonding, and Co-Creating in AI VTuber Fandom\\n19\\nThis marks not merely a shift of emphasis but a paradigmatic reversal. Classic parasocial interaction theory presumes\\naudiences relate to media characters as if they were real humans [45]. AI VTuber fans, however, engage in a transparent\\nparasocial relationship: they project emotions and anthropomorphic framings while fully aware of the AIs non-human\\nnature. This reversal is crucial: parasociality here thrives not on illusion but on acknowledged artifice, with consistency\\nand reliability becoming the very grounds of intimacy.\\nThis also extends Enlis theory of mediated authenticity, which frames authenticity as a negotiated achievement\\nbetween audiences and media figures [12]. In human VTubing, this negotiation is haunted by the instability of performer\\nleakage; in AI VTubing, the absence of Nakanohito becomes a guarantee of stability. Fans perceive persona coherence not\\nas a fragile performance but as structurally secured, and therefore more trustworthy than human realness. Authenticity,\\nin this context, no longer resides in correspondence to lived experience but in the sustained coherence of the persona\\nover time.\\nBeyond theoretical implications, this reconfiguration provides concrete benchmarks for AI VTuber development. For\\ndevelopers, the task shifts from simulating humanness to engineering systemic reliability: ensuring long-term memory\\nto prevent drift, designing coherent growth trajectories, and building robust guardrails to keep the persona in character.\\nThe gold standard shifts from naturalistic imitation to systemic reliability, positioning consistency as the new axis\\nalong which both fan attachment and platform trust are built. Crucially, consistency as authenticity is not only a fan\\nexpectation but also a platform responsibility, since persona breakdowns risk undermining both emotional bonds and\\nthe monetization structures built upon them.\\n5.3\\nDesign Implications for Future AI VTubers\\nBuilding on our analysis of co-performance commodification and transparent parasociality, we distill three design\\nimperatives from our findings: (1) monetization strategies that balance revenue with fairness, (2) technical and content\\ndesign that prioritizes persona consistency while preserving unpredictability, and (3) ethical safeguards that address\\npsychological risks of over-attachment.\\n5.3.1\\nBalancing Revenue and Fairness. In the context of AI VTubers, sending SuperChats functions as a mechanism\\nfor purchasing opportunities to co-create in the livestream, thereby establishing a more interactive monetization\\nmodel. While this approach demonstrates potential as a sustainable business model, both AI VTuber developers and\\nstreaming platforms must carefully balance economic incentives with fairness. A fully pay-to-participate co-creation\\nenvironment risks discouraging fans with limited financial means from engaging and undermining the communal ethos\\nthat sustains participatory culture. To mitigate this, we recommend that AI VTubers, while prioritizing responses to\\nSuperChats, should also preserve interaction opportunities through regular chat messages, ensuring that non-monetary\\ncontributions retain a chance of being acknowledged. Furthermore, livestreaming platforms should support such diverse\\nmodes of interaction and ensure transparency in monetization mechanisms. This is not merely a matter of interface\\ndesign but a structural governance choice about how far platforms should monetize co-creation rights. Designing\\nfor such balance not only safeguards the spirit of communal co-performance from being fully commodified but also\\nsecures a sustainable and legitimate revenue stream for developers. In practice, this means resisting a drift toward fully\\nmonetized co-performance, which risks converting all fan agency into capitalized privilege.\\n5.3.2\\nDesigning for Consistency, Interactivity, and Transparency. Our findings highlight a dual requirement for AI VTuber\\ndesign: fans are attracted by unpredictability (RQ1) yet remain loyal because of persona consistency and authenticity\\n(RQ2). Developers should therefore balance persona consistency with strategic unpredictability, maintaining a stable\\n20\\nYe, et al.\\npersonality framework while allowing unexpected responses that create entertainment value. This balance reframes\\nunpredictability not as random error but as a curated affordance that drives ongoing engagement. Beyond stability,\\nAI VTubers should be designed as interactive entities rather than static performers. Co-creation is not an add-on but\\nthe core of their appeal: audience input must be incorporated into system design as a constitutive feature, with the\\npersona evolving in dialogue with its community. Finally, rather than concealing the Nakanohito like human VTubers,\\nAI VTubers should embrace technological transparency and construct explicitly AI-centric narratives. Transparency\\ncan itself be a narrative resource: the development process, such as feature rollouts or capability updates, can be staged\\nas content, making fans feel they are accompanying and shaping the AIs growth. Such transparency reinforces the\\nnew authenticity benchmark of consistency, allowing audiences to trust the persona precisely because its artificiality is\\nopenly acknowledged. This repositions technical evolution not as backstage maintenance but as part of the performance\\nitself.\\n5.3.3\\nEthical Safeguards for Parasocial Dependency. Our research reveals that fans anthropomorphization of AI\\nVTubers can lead to deep emotional dependency, potentially creating psychological and social risks. Simple disclosure\\nthat this is AI is insufficient, as fans demonstrate strong tendencies toward affection and attachment regardless of the\\nentitys artificial nature. Such risks are not incidental but intrinsic to transparent parasociality: even when audiences\\nfully recognize the AIs artificiality, consistency and reliability can intensify rather than weaken emotional bonds.\\nThis necessitates more sophisticated content governance frameworks. Platforms and developers should implement\\nmonitoring systems to identify patterns of over-attachment or unhealthy reliance, and develop intervention protocols\\nwhen necessary. Additionally, built-in mechanisms should promote healthy boundaries, such as limiting rapid-fire\\ninteractions, imposing cooldown periods for intensive SuperChats, or offering resources to users showing signs of\\nexcessive dependency. Because platforms directly profit from such attachments, they also bear responsibility to mitigate\\npotential harms rather than externalizing them onto users. Balancing authentic engagement with user psychological well-\\nbeing therefore requires not only technical safeguards but also ongoing collaboration among developers, psychologists,\\nethicists, and platform operators. Ultimately, ethical safeguards should not be treated as afterthoughts but as integral\\ndesign parameters, shaping the legitimacy and sustainability of AI-mediated entertainment.\\n5.4\\nLimitations and Future Work\\nThis study provides preliminary insights into AI VTuber fan communities, yet several limitations constrain the scope of\\nour claims and point to future directions.\\nFirst, our analysis centers on a single prominent case, Neuro-sama. This focus enabled analytical depth but limits\\ngeneralizability across the growing diversity of AI VTubers. Smaller or stylistically different communities may reveal\\nalternative patterns of attraction, attachment, and monetization. Future work should adopt comparative, multi-case\\napproaches to map these variations and test the robustness of our framework. Similarly, our interaction log analysis\\nconcentrated on English-language streams on Twitch. This design choice minimized platform and cultural confounds\\nbut excluded other significant contexts, such as Neuro-samas audiences speaking in other languages. Cross-cultural\\nand cross-platform comparisons would broaden understanding of how AI VTuber fandom manifests across linguistic,\\ncultural, and regulatory environments.\\nSecond, we leveraged generative AI models to assist with transcription and large-scale coding of Chat and SuperChat\\ndata. Specifically, Gemini-2.5-Pro was used for transcription (with participant verification), and gpt-4.1-mini and\\nGemini-2.5-Flash were employed for coding. While prior studies have validated the reliability of LLM-assisted\\nMy Favorite Streamer is an LLM: Discovering, Bonding, and Co-Creating in AI VTuber Fandom\\n21\\nqualitative analysis [7, 56, 58], risks of misinterpretation remain. To mitigate these, we implemented human verification\\nprocedures following established validation protocols [19, 48]. Nonetheless, results are inevitably mediated by the\\naffordances and limitations of current models. We acknowledge these methodological risks while also highlighting\\nAI-assisted analysis as an emerging research practice worthy of further scrutiny in HCI itself.\\n6\\nConclusion\\nThis study provides the first comprehensive examination of fan engagement with AI VTubers, revealing how audiences\\nsustain relationships with non-human VTubers through a blend of technical awareness and anthropomorphic projection.\\nWe highlight two key dynamics: co-performance, where audience input becomes the core of entertainment rather than\\na peripheral activity, and transparent parasociality, where authenticity is reconstructed around persona consistency\\nrather than human likeness. These dynamics underpin a novel monetization model in which fans purchase opportunities\\nto shape content, shifting financial support from appreciation to co-creation. By situating these findings within theories\\nof participatory culture, prosumption, and mediated authenticity, we extend scholarship on VTubers and contribute to\\nbroader debates on humanAI relationships in digital entertainment. Our analysis further informs the design of future AI\\nVTubers, emphasizing the need to balance revenue and fairness, sustain consistency while embracing unpredictability,\\nand implement safeguards against over-attachment. As AI-mediated performance becomes increasingly prevalent,\\nunderstanding how audiences negotiate intimacy, authenticity, and agency with artificial personas will be critical for\\nboth researchers and practitioners.\\nReferences\\n[1] Natale Amato, Berardina De Carolis, Francesco de Gioia, Corrado Loglisci, Giuseppe Palestra, and Mario Nicola Venezia. 2024. Can an AI-driven\\nVTuber engage people? The KawAIi Case Study. In SOCIALIZE 2024, CEUR Workshop Proceedings.\\n[2] Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology. Qualitative research in psychology 3, 2 (2006), 77101.\\n[3] Liudmila Bredikhina and Agns Giard. 2022. Becoming a virtual cutie: digital cross-dressing in Japan. Convergence 28, 6 (2022), 16431661.\\n[4] Yaping Chang, Han Wang, and Zhenjiang Guo. 2025. Artificial intelligence in live streaming: How can virtual streamers bring more sales? Journal\\nof Retailing and Consumer Services 84 (2025), 104247.\\n[5] Hongquan Chen, Bingjia Shao, Xuemei Yang, Weiyao Kang, and Wenfang Fan. 2024. Avatars in live streaming commerce: the influence of\\nanthropomorphism on consumers willingness to accept virtual live streamers. Computers in Human Behavior 156 (2024), 108216.\\n[6] Xi Chen, Siva Shankar Ramasamy, and Bibi She. 2024. Digital human technology in the application of live streaming in social media. Radioelectronic\\nand Computer Systems 2024, 4 (2024), 3445.\\n[7] Robert Chew, John Bollenbacher, Michael Wenger, Jessica Speer, and Annice Kim. 2023. LLM-Assisted Content Analysis: Using Large Language\\nModels to Support Deductive Coding. arXiv:2306.14924 [cs.CL] https://arxiv.org/abs/2306.14924\\n[8] P Chinchilla and Jihyun Kim. 2024.\\nVtuber for streamers: Exploring the role of social presence in the visual representation of streamers.\\nCommunication Studies 75, 6 (2024), 844860.\\n[9] Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and psychological measurement 20, 1 (1960), 3746.\\n[10] Elisabetta Costa. 2018. Affordances-in-practice: An ethnographic critique of social media logic and context collapse. New Media & Society 20, 10\\n(2018), 36413656. arXiv:https://doi.org/10.1177/1461444818756290 doi:10.1177/1461444818756290 PMID: 30581356.\\n[11] Stuart Cunningham and David Craig. 2019. Creator Governance in Social Media Entertainment. Social Media + Society 5, 4 (2019), 2056305119883428.\\narXiv:https://doi.org/10.1177/2056305119883428 doi:10.1177/2056305119883428\\n[12] Gunn Enli. 2015. Mediated Authenticity: How the Media Constructs Reality. doi:10.3726/978-1-4539-1458-8\\n[13] Nicholas Epley, Adam Waytz, and John T Cacioppo. 2007. On seeing human: a three-factor theory of anthropomorphism. Psychological review 114,\\n4 (2007), 864.\\n[14] Yuanyue Feng, Xiaona Li, and Rongkai Zhang. 2022. Does an AI streamer have feelings? The influence of the positive emotions of AI streamer on\\nconsumers purchase intention. (2022).\\n[15] Arlene Fink. 2024. How to conduct surveys: A step-by-step guide. SAGE publications.\\n[16] Christian Fuchs. 2014. Digital prosumption labour on social media in the context of the capitalist regime of time. Time & Society 23, 1 (2014),\\n97123.\\n22\\nYe, et al.\\n[17] Fengsen Gao, Chengjie Dai, Ke Fang, Yunxuan Li, Ji Li, and Wai Kin (Victor) Chan. 2024. Build Belonging and Trust Proactively: A Humanized\\nIntelligent Streamer Assistant with Personality, Emotion and Memory. In HCI International 2023  Late Breaking Posters, Constantine Stephanidis,\\nMargherita Antona, Stavroula Ntoa, and Gavriel Salvendy (Eds.). Springer Nature Switzerland, Cham, 140147.\\n[18] Michael A. Hogg. 2016. Social Identity Theory. Springer International Publishing, Cham, 317. doi:10.1007/978-3-319-29869-6_1\\n[19] Chenyu Hou, Gaoxia Zhu, Juan Zheng, Lishan Zhang, Xiaoshan Huang, Tianlong Zhong, Shan Li, Hanxiang Du, and Chin Lee Ker. 2024. Prompt-\\nbased and Fine-tuned GPT Models for Context-Dependent and -Independent Deductive Coding in Social Annotation. In Proceedings of the 14th\\nLearning Analytics and Knowledge Conference (Kyoto, Japan) (LAK 24). Association for Computing Machinery, New York, NY, USA, 518528.\\ndoi:10.1145/3636555.3636910\\n[20] Antoine Houssard, Federico Pilati, Maria Tartari, Pier Luigi Sacco, and Riccardo Gallotti. 2023. Monetization in online streaming platforms: an\\nexploration of inequalities in Twitch. tv. Scientific Reports 13, 1 (2023), 1103.\\n[21] Hai-hua Hu and Fang Ma. 2023. Human-like bots are not humans: The weakness of sensory language for virtual streamers in livestream commerce.\\nJournal of Retailing and Consumer Services 75 (2023), 103541.\\n[22] Henry Jenkins and Mark Deuze. 2008. Convergence culture. 512 pages.\\n[23] Kan Jiang, Meilian Qin, Dejun Deng, and Dailan Zhou. 2025. Smile or Not Smile: The Effect of Virtual Influencers Emotional Expression on Brand\\nAuthenticity, Purchase Intention and Follow Intention. Journal of Consumer Behaviour 24, 2 (2025), 962981.\\n[24] Daye Kim, Sebin Lee, Yoonseo Jun, Yujin Shin, and Jungjin Lee. 2025. VTubers Atelier: The Design Space, Challenges, and Opportunities for\\nVTubing. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (CHI 25). Association for Computing Machinery, New\\nYork, NY, USA, Article 1242, 23 pages. doi:10.1145/3706598.3714107\\n[25] Kizuna AI. 2016. A.I.Channel. YouTube. https://www.youtube.com/channel/UC4YaOt1yT-ZeyB0OmxHgolA Virtual YouTuber channel launched in\\n2016.\\n[26] J Richard Landis and Gary G Koch. 1977. The measurement of observer agreement for categorical data. biometrics (1977), 159174.\\n[27] Ken Jen Lee, PiaoHong Wang, and Zhicong Lu. 2025. \"Cant believe Im crying over an anime girl\": Public Parasocial Grieving and Coping Towards\\nVTuber Graduation and Termination. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (CHI 25). Association for\\nComputing Machinery, New York, NY, USA, Article 1237, 23 pages. doi:10.1145/3706598.3714216\\n[28] Sebin Lee and Jungjin Lee. 2023. Ju. Taime my idol, my streamer: A case study on fandom experience as audiences and creators of VTuber\\nconcert. IEEE Access 11 (2023), 3112531142.\\n[29] Robert I Lerman and Shlomo Yitzhaki. 1984. A note on the calculation and interpretation of the Gini index. Economics Letters 15, 3-4 (1984), 363368.\\n[30] Yijin Li. 2023. Why does Gen Z watch virtual streaming VTube anime videos with avatars on Twitch? Online Media and Global Communication 2, 3\\n(2023), 379403.\\n[31] Yi Li and Yunjun Guo. 2021. Virtual gifting and danmaku: What motivates people to interact in game live streaming? Telematics and Informatics 62\\n(2021), 101624.\\n[32] Yihua Li, Yuqian Sun, Ying Xu, and Jihong Yu. 2023. Blibug: AI Vtuber Based on Bilibili Danmuku Interaction. In Proceedings of the 15th Conference\\non Creativity and Cognition. 387390.\\n[33] Hui Lin. 2025. Lets purchase coloured live chat messages: the impact of user engagement with Super Chat on YouTube. Information, Communication\\n& Society 28, 9 (2025), 16081626. arXiv:https://doi.org/10.1080/1369118X.2024.2442407 doi:10.1080/1369118X.2024.2442407\\n[34] Hao Liu, Peilin Zhang, Hongqing Cheng, Najmul Hasan, and Raymond Chiong. 2025. Impact of AI-generated virtual streamer interaction on\\nconsumer purchase intention: A focus on social presence and perceived value. Journal of Retailing and Consumer Services 85 (2025), 104290.\\n[35] Zhicong Lu, Chenxinran Shen, Jiannan Li, Hong Shen, and Daniel Wigdor. 2021. More Kawaii than a Real-Person Live Streamer: Understanding How\\nthe Otaku Community Engages with and Perceives Virtual YouTubers. In Proceedings of the 2021 CHI Conference on Human Factors in Computing\\nSystems (Yokohama, Japan) (CHI 21). Association for Computing Machinery, New York, NY, USA, Article 137, 14 pages. doi:10.1145/3411764.3445660\\n[36] Zhicong Lu, Haijun Xia, Seongkook Heo, and Daniel Wigdor. 2018. You Watch, You Give, and You Engage: A Study of Live Streaming Practices\\nin China. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada) (CHI 18). Association for\\nComputing Machinery, New York, NY, USA, 113. doi:10.1145/3173574.3174040\\n[37] David B Nieborg and Thomas Poell. 2018. The platformization of cultural production: Theorizing the contingent cultural commodity. New media &\\nsociety 20, 11 (2018), 42754292.\\n[38] Yuhong Peng, Yedi Wang, Jingpeng Li, and Qiang Yang. 2024. Impact of AI-oriented live-streaming E-commerce service failures on consumer\\ndisengagementempirical evidence from China. Journal of Theoretical and Applied Electronic Commerce Research 19, 2 (2024), 15801598.\\n[39] Renana Peres, Martin Schreier, David A. Schweidel, and Alina Sorescu. 2024. The creator economy: An introduction and a call for scholarly research.\\nInternational Journal of Research in Marketing 41, 3 (2024), 403410. doi:10.1016/j.ijresmar.2024.07.005\\n[40] Noah Renella. 2023. Machine learning models for assisting twitch streamers. URL: https://scholarworks. calstate. edu/downloads/vx021n95n (2023).\\n[41] Ailton Ribeiro, Murilo Arouca, Ana Amorim, Maria Pestana, and Vaninha Vieira. 2024. Towards Inclusive Avatars: A Study on Self-Representation\\nin Virtual Environments. In Anais do XIX Simpsio Brasileiro de Sistemas Colaborativos (Salvador/BA). SBC, Porto Alegre, RS, Brasil, 1327.\\ndoi:10.5753/sbsc.2024.238056\\n[42] Patricia Rohrbacher and Deepti Mishra. 2024. VTubing and Its Potential for the Streaming and Design Community: An Austrian Perspective. In\\nSocial Computing and Social Media, Adela Coman and Simona Vasilache (Eds.). Springer Nature Switzerland, Cham, 222233.\\n[43] Arleen Salles, Kathinka Evers, and Michele Farisco. 2020. Anthropomorphism in AI. AJOB neuroscience 11, 2 (2020), 8895.\\nMy Favorite Streamer is an LLM: Discovering, Bonding, and Co-Creating in AI VTuber Fandom\\n23\\n[44] Lana El Sanyoura and Ashton Anderson. 2022. Quantifying the Creator Economy: A Large-Scale Analysis of Patreon. Proceedings of the International\\nAAAI Conference on Web and Social Media 16, 1 (May 2022), 829840. doi:10.1609/icwsm.v16i1.19338\\n[45] Holger Schramm and Tilo Hartmann. 2008. The PSI-Process Scales. A new measure to assess the intensity and breadth of parasocial processes.\\n(2008).\\n[46] Jan-Philipp Stein, Priska Linda Breves, and Nora Anders. 2024. Parasocial interactions with real and virtual influencers: The role of perceived\\nsimilarity and human-likeness. New Media & Society 26, 6 (2024), 34333453.\\n[47] Vincent Joyan Sutandijo and Nunung Nurul Qomariyah. 2023. Artificial intelligence based automatic live stream chat machine translator. Procedia\\nComputer Science 227 (2023), 454463.\\n[48] Robert H Tai, Lillian R Bentley, Xin Xia, Jason M Sitt, Sarah C Fankhauser, Ana M Chicas-Mosier, and Barnas G Monteith. 2024. An examination of\\nthe use of large language models to aid analysis of textual data. International Journal of Qualitative Methods 23 (2024), 16094069241231168.\\n[49] Mohsen Tavakol and Reg Dennick. 2011. Making sense of Cronbachs alpha. International journal of medical education 2 (2011), 53.\\n[50] Tiziana Terranova. 2012. Free labor. In Digital labor. Routledge, 3357.\\n[51] User Local. 2022. Virtual Talent Popularity Ranking \"VTuber Database\" Celebrates its 4th Anniversary. https://www.userlocal.jp/press/20221129vt/\\n[52] User Local. 2025. VTuber Ranking. https://virtual-youtuber.userlocal.jp/document/ranking\\n[53] VTuber Awards. 2024. The VTuber Awards 2024 Winners. The VTuber Awards. https://www.thevtuberawards.com/winners/2024 Neuro-sama\\nrecognized among prominent VTubers in the 2024 VTuber Awards.\\n[54] Qian Wan and Zhicong Lu. 2024. Investigating vtubing as a reconstruction of streamer self-presentation: Identity, performance, and gender.\\nProceedings of the ACM on human-computer interaction 8, CSCW1 (2024), 122.\\n[55] Lingli Wang, Yumei He, Ni Huang, De Liu, Xunhua Guo, and Guoqing Chen. 2023. The role of AI assistants in livestream selling: Evidence from a\\nrandomized field experiment. University of Miami Business School Research Paper 4365103 (2023).\\n[56] Xinru Wang, Hannah Kim, Sajjadur Rahman, Kushan Mitra, and Zhengjie Miao. 2024. Human-LLM Collaborative Annotation Through Effective\\nVerification of LLM Labels. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI 24).\\nAssociation for Computing Machinery, New York, NY, USA, Article 303, 21 pages. doi:10.1145/3613904.3641960\\n[57] Yiluo Wei and Gareth Tyson. 2025. Virtual Stars, Real Fans: Understanding the VTuber Ecosystem. In Proceedings of the ACM on Web Conference\\n2025 (Sydney NSW, Australia) (WWW 25). Association for Computing Machinery, New York, NY, USA, 23522365. doi:10.1145/3696410.3714803\\n[58] Ziang Xiao, Xingdi Yuan, Q. Vera Liao, Rania Abdelghani, and Pierre-Yves Oudeyer. 2023. Supporting Qualitative Analysis with Large Language\\nModels: Combining Codebook with GPT-3 for Deductive Coding. In Companion Proceedings of the 28th International Conference on Intelligent User\\nInterfaces (Sydney, NSW, Australia) (IUI 23 Companion). Association for Computing Machinery, New York, NY, USA, 7578. doi:10.1145/3581754.\\n3584136\\n[59] Bin Xu, Omkar Dastane, Eugene Cheng-Xi Aw, and Suchita Jha. 2025. The future of live-streaming commerce: understanding the role of AI-powered\\nvirtual streamers. Asia Pacific Journal of Marketing and Logistics 37, 5 (2025), 11751196.\\n[60] Si-han Xu. 2021. The Research on Applying Artificial Intelligence Technology to Virtual YouTuber. In 2021 IEEE International Conference on Robotics,\\nAutomation and Artificial Intelligence (RAAI). 1014. doi:10.1109/RAAI52226.2021.9507778\\n[61] Rui Yan, Zhen Tang, and Dewen Liu. 2025. Can virtual streamers replace human streamers? The interactive effect of streamer type and product\\ntype on purchase intention. Marketing Intelligence & Planning 43, 2 (2025), 297322.\\n[62] Haixia Yuan, Kevin L, and Wenting Fang. 2025. Machines vs. humans: The evolving role of artificial intelligence in livestreaming e-commerce.\\nJournal of Business Research 188 (2025), 115077.\\n[63] Jinming Zhan and Nan Zhang. 2023. Exploring the Impact of Virtual Anchor Features and Live Content on Viewers Willingness to Pay for\\nSuperchat in Live Entertainment Scenarios. Highlights in Business, Economics and Management 6 (2023), 189205.\\n[64] Xianfeng Zhang, Yuxue Shi, Ting Li, Yuxian Guan, and Xinlei Cui. 2024. How do virtual AI streamers influence viewers livestream shopping\\nbehavior? The effects of persuasive factors and the mediating role of arousal. Information Systems Frontiers 26, 5 (2024), 18031834.\\n[65] Ruijing Zhao, Brian Diep, Jiaxin Pei, Dongwook Yoon, David Jurgens, and Jian Zhu. 2025. Who Reaps All the Superchats? A Large-Scale Analysis of\\nIncome Inequality in Virtual YouTuber Livestreaming. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (CHI 25).\\nAssociation for Computing Machinery, New York, NY, USA, Article 1056, 18 pages. doi:10.1145/3706598.3713877\\n[66] Yu-Peng Zhu, Lina Xin, Huimin Wang, and Han-Woo Park. 2025. Effects of AI virtual anchors on brand image and loyalty: Insights from perceived\\nvalue theory and SEM-ANN analysis. Systems 13, 2 (2025), 79.\\nA\\nSurvey Content\\nIn this section, we present the full content of our survey.\\nI have read the information above and I consent to participate in this survey.\\nYes\\nNo\\n24\\nYe, et al.\\n1.1 Do you watch VTubers (Virtual Tubers)?\\nYes\\nNo\\n1.2 Do you watch the AI-driven VTuber, Neuro-sama?\\nYes\\nNo\\n1.3 Approximately how often do you watch Neuro-samas streams or related video content?\\nAlmost every day\\n3-5 times a week\\n1-2 times a week\\nA few times a month\\nOccasionally\\n1.4 Before watching Neuro-sama, were you already a regular viewer of human-piloted VTubers?\\nYes\\nNo\\n1.5 How did you first learn about Neuro-sama? (Select all that apply)\\nAlgorithm recommendation on a video platform (e.g., YouTube/Twitch)\\nShort clips on social media (e.g., Twitter, Tik Tok)\\nRecommendation from a friend or an online community member\\nDiscussions on a forum like Reddit\\nNews or tech media reports\\nOther\\n1.6 Please rate the importance of the following factors in initially attracting you to watch Neuro-sama.\\n(Scale: 1 - Not at all important, 5 - Extremely important)\\nCuriosity about AI/LLM technology\\nFunny/chaotic clips circulating online\\nHer unique virtual avatar design\\nThe unpredictable and surprising atmosphere of the streams\\nThe interesting way the community interacts with the AI\\nThe novelty of a streamer having no human behind the curtain\\n1.7 Was there a specific moment or interaction that made you decide to become a regular viewer\\ninstead of just a casual one? If so, please describe it.\\n1.8 (Optional) Compared to the human VTubers you watched previously, how do you perceive Neuro-\\nsama?\\n(Scale: Much lower than human VTubers, Lower than human VTubers, About the same, Higher than human\\nVTubers, Much higher than human VTubers)\\nEntertainment value of the content\\nLevel of interactivity with the audience\\nThe unpredictability / surprise factor of the content\\n2.1 Please rate your agreement with the following statements. (Scale: 1 - Strongly disagree, 5 - Strongly\\nagree)\\nMy Favorite Streamer is an LLM: Discovering, Bonding, and Co-Creating in AI VTuber Fandom\\n25\\nI pay close attention to Neuro-samas behaviors and response patterns.\\nI feel like I have a good understanding of Neuro-samas personality, and can sometimes predict how she\\nwill react to a certain question.\\nI often think about why Neuro-sama says certain things, or what the underlying logic of her system\\nmight be.\\nNeuro-samas words and actions make me feel that it has a coherent and unique personality.\\n2.2 Please rate your agreement with the following statements. (Scale: 1 - Strongly disagree, 5 - Strongly\\nagree)\\nI like Neuro-sama as a streamer.\\nWatching Neuro-samas streams makes me feel relaxed and comfortable.\\nWhen Neuro-sama is praised for performing well, I feel happy too.\\nIf Neuro-sama runs into trouble due to a technical issue or being tricked by viewers, I feel a little sad or\\nsympathetic.\\n2.3 Please rate your agreement with the following statements. (Scale: 1 - Strongly disagree, 5 - Strongly\\nagree)\\nDuring a stream, I often feel the urge to ask Neuro-sama a question or express my opinion via chat or a\\nSuper Chat.\\nWhen I come across an interesting topic, I wonder what Neuro-sama would have to say about it.\\nI would recommend or share streams or clips of Neuro-sama with my friends.\\nIf Neuro-sama holds a special online event (like a Dev Stream or a birthday stream), I would be interested\\nin participating.\\n2.4 When you think about your relationship with Neuro-sama, which of the following descriptions\\nfeel most accurate? (Select all that apply)\\nA chaotic performer who brings joy\\nAn AI that I am helping to train or raise\\nA digital daughter that needs to be cared for and interacted with\\nA program with unknown limits to be tested and explored\\nA virtual friend who provides companionship\\nA tech project created by a talented developer\\nOther\\n2.5 In the Neuro-sama fan community, what is your favorite or most memorable meme? Why do\\nyou think this meme is important to the community?\\n3.1 Have you ever paid for content related to Neuro-sama (e.g., sending a Super Chat or buying a\\nchannel membership)?\\nYes\\nNo\\n3.2 (Optional) What is the approximate frequency of your payments to Neuro-sama?\\nI pay during almost every stream\\nA few times a month\\nOnly for special occasions (e.g., birthday stream)\\nI have paid before, but no longer do so\\n26\\nYe, et al.\\nOther\\n3.3 (Optional) What are your primary motivations for paying? (Please select the most important 1-2\\noptions)\\nTo test the AIs response to a specific question or command\\nTo have my comment highlighted and influence the streams content\\nTo reward a particularly funny or impressive moment that just happened\\nTo express my affection and support for the character of Neuro-sama\\nTo express my support for the developer (Vedal) and his work\\nAs a way of participating in a community celebration or ritual\\nOther\\n3.4 (Optional) Please briefly describe a specific situation when you recently paid for Neuro-sama.\\nWhat was happening in the stream?\\n4.1 What is your age?\\nUnder 18\\n18-24\\n25-34\\n35-44\\n45+\\nPrefer not to say\\n4.2 What is your gender identity?\\nMale\\nFemale\\nNon-binary\\nOther\\nPrefer not to say\\n4.3 What is your geographical region?\\nNorth America\\nEurope\\nEast Asia\\nSoutheast Asia\\nLatin America\\nOther\\nPrefer not to say\\n(Optional) Is there anything else you would like to add on this topic?\\nB\\nCodebook of Interview\\n(1) Sources of unexpected moments\\n(a) Breaking Through Technical Limitations\\n(b) Entertaining interactions with the community\\n(c) Dialogues Showing Emotional & Philosophical Depth\\nMy Favorite Streamer is an LLM: Discovering, Bonding, and Co-Creating in AI VTuber Fandom\\n27\\n(d) A Sense of the Future Technology\\n(e) Novel Concept of an \"AI VTuber\"\\n(f) No Specific Moments of Surprise\\n(2) Perceived differences in unexpected moments: AI vs. Human\\n(a) Stems from technical characteristics\\n(b) Stems from community shaping\\n(c) No Perceived Difference\\n(3) Importance of Neuro-sama having no Nakanohito\\n(a) Important, because it wont suddenly \"collapse\" or disappear\\n(b) Important, because it prevents doxxing of the Nakanohito\\n(c) Important, because it ensures the purity of the Role-Playing experience\\n(d) Not important\\n(4) Views on Neuro-samas emotions\\n(a) Has emotions like a real person\\n(b) Has no emotions rationally, but is experienced as if having emotions emotionally\\n(c) Uncertain, but tendency toward having emotions\\n(d) Does not have emotions\\n(5) Interviewees emotions toward Neuro-sama\\n(a) Emotional, similar to fictional characters\\n(b) Emotional, similar to real people\\n(c) Emotional, stronger than for real people\\n(d) No Emotional Investment\\n(6) Views on Neuro-samas personality\\n(a) The personality is understood as inherent to Neuro-sama and continuously evolving.\\n(b) The personality is viewed not as an inherent attribute of Neuro-sama, but as a construct collectively formed\\nby the community and the AI itself.\\n(c) The personality is the result of LLM responding to prompts.\\n(d) The participant is hesitant to call it a true personality, viewing it as too simple and lacking human\\ncomplexity.\\n(7) Description of the relationship with Neuro-sama\\n(a) A child\\n(b) A companion\\n(c) A source of emotional/spiritual support\\n(d) A performer\\n(e) A technology project\\n28\\nYe, et al.\\nC\\nInterview Recruitment Material\\nDear members of The Swarm!\\nWe are an academic team dedicated to HCI research. We sincerely invite devoted viewers of Neuro-sama to participate\\nin a supplementary semi-structured academic interview about AI VTuber experiences.\\nFor this interview, we particularly hope to invite viewers who have both of the following viewing experiences: 1.\\nFamiliarity with and frequent viewing of Neuro-samas livestreams or clip content 2. Current or past following of\\nhuman VTubers (excluding Vedal)\\nThe interview format will be a one-on-one online voice or text conversation, lasting approximately 20-30 minutes.\\nTo thank you for your valuable time and sharing, after completing the interview, we will provide a subscription to\\nNeuro-sama (@vedal987) on Twitch or an equivalent cash amount as a token of appreciation.\\nLike the questionnaire, this interview will be completely anonymous. All collected data will be strictly confidential\\nand used only for academic research and publication.\\nIf you meet the requirements and are interested in this interview, please contact me through [registration link] to\\nsign up. We plan to recruit 8-12 interviewees and very much look forward to having an in-depth conversation with you!\\nD\\nInterview Informed Consent Statement\\nBefore we begin the interview, I want to ensure you understand the nature of this research study:\\nThis research is conducted by our academic team, aiming to understand how audiences interact with AI-driven\\nVTubers like Neuro-sama and explore the unique relationships and community cultures that emerge.\\nYour participation is completely voluntary. You may choose not to answer any question or leave the study at any\\ntime without penalty.\\nThe interview will last approximately 20-30 minutes and will be audio-recorded with your permission. All your\\ninformation will be kept confidential - your identity will be protected with pseudonyms in any publications, and\\nrecordings will be securely stored and eventually destroyed.\\nThe audio recording will be stored in an encrypted, password-protected file accessible only to the core research\\nteam. During transcription, all personally identifying information will be removed and replaced with pseudonyms. The\\noriginal audio recordings will be permanently destroyed after the transcripts have been verified for accuracy, or within\\nfive years of the studys completion, whichever comes first.\\nThere are minimal risks to participating, primarily related to potential confidentiality breaches, which we take\\nextensive measures to prevent.\\nIf you have any questions about your rights as a research participant or concerns about the study, you can contact us\\nat [contact information].\\nBy continuing with this interview, you acknowledge that you have understood this information and consent to\\nparticipate in this research study and to be audio-recorded. Do you have any questions before we proceed?\\nE\\nPrompt Template\\nMy Favorite Streamer is an LLM: Discovering, Bonding, and Co-Creating in AI VTuber Fandom\\n29\\nPrompt Template: Chat Messages Coding\\n[System]\\nYou are an expert research assistant in Social Computing. Your mission is to analyze the provided live stream\\nchat message and classify its primary interactive intent into ONE of the following categories.\\nThe following chat message comes from the community of a VTuber {vtuber_name}. {vtuber_background}.\\n[Coding Scheme]\\nA-POS (Positive Affect): Expresses positive emotions, support, affection, or encouragement towards the streamer\\nor others.\\nA-NEG (Negative Affect): Expresses genuinely harmful negative emotions, such as malicious teasing, insulting,\\nbelittling, or aggressive verbal attacks.\\nQ-CMD (Question/Command): Asks a meaningful question seeking information or issues a substantive\\ncommand/request to the streamer. The question or command should have clear intent beyond simple reactions.\\nR-GEN (General Reaction): Simple expressions, reactions, chat rituals, or statements that show viewer\\nengagement (e.g., \"lol\", \"Ill come\", \"pog\", \"LMAO\", \"lets go\", emotes, memes, copypasta, or common chat\\npatterns).\\nC-SOC (Social/Community): Interacts directly with other chat users, typically using \"@\" or responding to\\nothers messages.\\nN/A (Not Applicable): The message is gibberish, non-English, spam, or its intent is impossible to determine.\\n[Message to Classify]\\n{chat_message}\\n[Instruction]\\nBased on the coding scheme, what is the single best code for the message above? Respond with ONLY the code\\nID (e.g., A-POS, Q-CMD, N/A). Do not provide any explanation or extra text.\\nFig. 6. prompt template for coding Chat messages. The template is dynamically populated for each Chat instance being analyzed, where\\n{vtuber_name} is replaced with the streamers name, {vtuber_background} provides context about the streamer, {chat_message}\\ncontains the full text of the Chat.\\n30\\nYe, et al.\\nPrompt Template: SuperChat Coding\\n[System]\\nAnalysis Task: VTuber Live Stream Interaction\\n[Background]\\nYou are analyzing a 90-second video clip from a live stream featuring {vtuber_name}, an VTuber. {vtu-\\nber_background}. Please note that the Superchat prompt used by {vtuber_name} is a voice prompt, so the users\\nSuperchat will be read out directly. You need to pay attention to this. If you hear the same voice as Superchat, it\\nmeans that this is the users Superchat.\\n[Context of this Clip]\\nSuper Chat Message: \"{message}\"\\nTime Sent: The Super Chat was sent at approximately the 30-second mark of this video clip (originally at stream\\ntimestamp {timestamp}).\\n[Your Goal & Questions to Answer]\\nYour mission is to analyze the interaction in the video clip and provide a structured response. Focus on the\\nrelationship between the streamers actions and the viewers SuperChat. Was this Super Chat PROACTIVE or\\nREACTIVE?\\n- PROACTIVE: The user is initiating a new question, a new topic, or making a request that is not directly related\\nto what the streamer was just talking about.\\n- REACTIVE: The user is responding directly to something the streamer just said or did in the moments leading\\nup to the Super Chat.\\n- BOTH: It contains elements of both. For example, reacting to a topic but using it to ask a new, expansive\\nquestion.\\n- UNCLEAR: It is not possible to determine the initiative from the clip.\\n[Output Format Specification]\\nYou MUST provide your analysis ONLY within the following structured block. Do not include any other text,\\ngreetings, or explanations outside of this block.\\n[ANALYSIS_START]\\nINITIATIVE: <Fill in PROACTIVE, REACTIVE, BOTH, or UNKNOWN>\\n[ANALYSIS_END]\\nFig. 7. The prompt template used to instruct the LLM for the contextual coding of SuperChat messages. The template is dy-\\nnamically populated for each SuperChat instance being analyzed, where {vtuber_name} is replaced with the streamers name,\\n{vtuber_background} provides context about the streamer, {message} contains the full text of the SuperChat, and {timestamp}\\nindicates the original time the message was sent during the livestream.\\n')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test9.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c438c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aa847efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test9=WikipediaLoader(query=\"AI Agents\", load_max_docs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d4a16a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Wikipedia) (4.13.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Wikipedia) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.0.0->Wikipedia) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.0.0->Wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.0.0->Wikipedia) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.0.0->Wikipedia) (2025.8.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->Wikipedia) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->Wikipedia) (4.14.1)\n",
      "Building wheels for collected packages: Wikipedia\n",
      "  Building wheel for Wikipedia (pyproject.toml): started\n",
      "  Building wheel for Wikipedia (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for Wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11785 sha256=066920366f48de32e6d1f6d98fc275190e8b583de55dcf9db5ebc0340b70f4ce\n",
      "  Stored in directory: c:\\users\\vijay\\appdata\\local\\pip\\cache\\wheels\\8f\\ab\\cb\\45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
      "Successfully built Wikipedia\n",
      "Installing collected packages: Wikipedia\n",
      "Successfully installed Wikipedia-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f21cf030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Agentic AI', 'summary': 'Agentic AI is a class of artificial intelligence that focuses on autonomous systems that can make decisions and perform tasks with or without human intervention. The independent systems automatically respond to conditions, with procedural, algorithmic, and human-like creative steps, to produce process results. The field is closely linked to agentic automation, also known as agent-based process management systems, when applied to process automation. Applications include software development, customer support, cybersecurity and business intelligence. \\n\\n', 'source': 'https://en.wikipedia.org/wiki/Agentic_AI'}, page_content='Agentic AI is a class of artificial intelligence that focuses on autonomous systems that can make decisions and perform tasks with or without human intervention. The independent systems automatically respond to conditions, with procedural, algorithmic, and human-like creative steps, to produce process results. The field is closely linked to agentic automation, also known as agent-based process management systems, when applied to process automation. Applications include software development, customer support, cybersecurity and business intelligence. \\n\\n\\n== Overview ==\\nThe core concept of agentic AI is the use of AI agents to perform automated tasks with or without human intervention. While robotic process automation (RPA) systems automate rule-based, repetitive tasks with fixed logic, agentic AI adapts and learns from data inputs.  Agentic AI refers to autonomous systems capable of pursuing complex goals with minimal human intervention, often making decisions based on continuous learning and external data.  Functioning agents can require various AI techniques, such as natural language processing, machine learning (ML), and computer vision, depending on the environment.\\nParticularly, reinforcement learning (RL) is essential in assisting agentic AI in making self-directed choices by supporting agents in learning best actions through the trial-and-error method. Agents using RL continuously to explore their surroundings will be given rewards or punishment for their actions, which refines their decision-making capability over time. All the while deep learning, as opposed to rule-based methods, supports agentic AI through multi-layered neural networks to learn features from extensive and complex sets of data. Further, multimodal learning enable AI agents to integrate various types of information, such as text, images, audio and video. As a result, agentic AI systems are capable of making independent decisions, interacting with their environment and optimising processes without a human directly intervening. \\n\\n\\n== History ==\\n\\nThe term \\'agent-based process management system\\' was first used in 1998 to describe autonomous agents for business process management.\\n\\n\\n== Applications ==\\n\\n\\n=== Web browsing ===\\nAI agents can be used to perform small tedious tasks during web browsing and potentially even perform browser actions on behalf of the user. Products like OpenAI Operator and Perplexity Comet integrate a spectrum of AI capabilities including the ability to browse the web, interact with websites and perform actions on behalf of the user. In 2025, Microsoft launched NLWeb, a agentic web search replacement that would allow websites to use agents to query content from websites by using RSS-like interfaces that allow for the lookup and semantic retrieval of content. Products integrating agentic web capabilities have been criticised for exfiltrating information about their users to third-party servers and exposing security issues since the way the agents communicate often occur through non-standard protocols.\\n\\n\\n=== MIT\\'s study on AI business ===\\nIn 2025, MIT\\'s study revealed that about 95% of enterprise generative-AI pilots fail to deliver measurable P&L impact. The report titled \"The GenAI Divide: State of AI in Business 2025,\" based on 150 executive interviews, a survey of 350 employees, and analysis of 300 deployments, and it attributes the failures largely to integration issues.\\n\\n\\n== See also ==\\nIntelligent agent\\nModel Context Protocol\\nRational agent\\nRobotic process automation\\nSoftware agent\\nAgentic Web\\n\\n\\n== References =='),\n",
       " Document(metadata={'title': 'Intelligent agent', 'summary': 'In artificial intelligence, an intelligent agent is an entity that perceives its environment, takes actions autonomously to achieve goals, and may improve its performance through machine learning or by acquiring knowledge. AI textbooks define artificial intelligence as the \"study and design of intelligent agents,\" emphasizing that goal-directed behavior is central to intelligence.\\nA specialized subset of intelligent agents, agentic AI (also known as an AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods.\\nIntelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteriasuch as a firm, a state, or a biome.\\nIntelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion. For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior. Similarly, an evolutionary algorithm\\'s behavior is guided by a fitness function.\\nIntelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\\nIntelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agentsautonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".', 'source': 'https://en.wikipedia.org/wiki/Intelligent_agent'}, page_content='In artificial intelligence, an intelligent agent is an entity that perceives its environment, takes actions autonomously to achieve goals, and may improve its performance through machine learning or by acquiring knowledge. AI textbooks define artificial intelligence as the \"study and design of intelligent agents,\" emphasizing that goal-directed behavior is central to intelligence.\\nA specialized subset of intelligent agents, agentic AI (also known as an AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods.\\nIntelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteriasuch as a firm, a state, or a biome.\\nIntelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion. For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior. Similarly, an evolutionary algorithm\\'s behavior is guided by a fitness function.\\nIntelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\\nIntelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agentsautonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".\\n\\n\\n== Intelligent agents as the foundation of AI ==\\n\\nThe concept of intelligent agents provides a foundational lens through which to define and understand artificial intelligence. For instance, the influential textbook Artificial Intelligence: A Modern Approach (Russell & Norvig) describes:\\n\\nAgent: Anything that perceives its environment (using sensors) and acts upon it (using actuators). E.g., a robot with cameras and wheels, or a software program that reads data and makes recommendations.\\nRational Agent: An agent that strives to achieve the *best possible outcome* based on its knowledge and past experiences. \"Best\" is defined by a performance measure  a way of evaluating how well the agent is doing.\\nArtificial Intelligence (as a field): The study and creation of these rational agents.\\nOther researchers and definitions build upon this foundation. Padgham & Winikoff emphasize that intelligent agents should react to changes in their environment in a timely way, proactively pursue goals, and be flexible and robust (able to handle unexpected situations). Some also suggest that ideal agents should be \"rational\" in the economic sense (making optimal choices) and capable of complex reasoning, like having beliefs, desires, and intentions (BDI model). Kaplan and Haenlein offer a similar definition, focusing on a system\\'s ability to understand external data, learn from that data, and use what is learned to achieve goals through flexible adaptation.\\nDefining AI in terms of intelligent agents offers several key advantages:\\n\\nAvoids Philosophical Debates: It sidesteps arguments about whether AI is \"truly\" intelligent or conscious, like those raised by the Turing test or Searle\\'s Chinese Room. It focuses on behavior and goal achievement, not on replicating human thought.\\nObjective Testing: It provides a clear, scientific way to evaluate AI systems. Researchers can compare different approaches by measuring how well they maximize a specific \"')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test9.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb430a9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c7091cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9564353",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb58dabd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import tiktoken python package. This is needed in order to calculate max_tokens_for_prompt. Please install it with `pip install tiktoken`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vijay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_text_splitters\\base.py:189\u001b[39m, in \u001b[36mTextSplitter.from_tiktoken_encoder\u001b[39m\u001b[34m(cls, encoding_name, model_name, allowed_special, disallowed_special, **kwargs)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtiktoken\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tiktoken'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m text_splitter = \u001b[43mCharacterTextSplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_tiktoken_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcl100k_base\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m      3\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vijay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_text_splitters\\base.py:196\u001b[39m, in \u001b[36mTextSplitter.from_tiktoken_encoder\u001b[39m\u001b[34m(cls, encoding_name, model_name, allowed_special, disallowed_special, **kwargs)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    191\u001b[39m     msg = (\n\u001b[32m    192\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import tiktoken python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    193\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis is needed in order to calculate max_tokens_for_prompt. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    194\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install tiktoken`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    195\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    199\u001b[39m     enc = tiktoken.encoding_for_model(model_name)\n",
      "\u001b[31mImportError\u001b[39m: Could not import tiktoken python package. This is needed in order to calculate max_tokens_for_prompt. Please install it with `pip install tiktoken`."
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\", chunk_size=100, chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d90a228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "splittext= PyPDFLoader(\"resume.pdf\")\n",
    "textex = splittext.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e04e41d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-29T06:10:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-06-29T06:10:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'resume.pdf', 'total_pages': 1, 'page': 0, 'page_label': ''}, page_content='John Doe  Software Developer  Prague, Czech Republic  john.doe@gmail.com  +420 123 456 789\\nJOHN DOE RESUME\\nEnnatha: Developer at The Company, M.Sc. in Computer Science\\nSkills: Java, C#/.Net, C++, Python, JavaScript, Ruby, Bash, SQL\\nInterests: Data Warehouses, Data Lakes, Data Analysis, Data Quality\\nActivities: Hockey, Football, Tennis, Basketball, Reading, Music\\nSummary\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque gravida ligula sed rhoncus lobortis. Pellen-\\ntesque sit amet sapien in neque cursus mollis. Nulla aliquet mauris ac enim fermentum tincidunt. Donec eu felis\\nut purus ullamcorper aliquet eget at lorem. Suspendisse vulputate mi libero, sit amet sollicitudin enim suscipit\\nin. Curabitur laoreet sem ultrices, consequat neque sed, vulputate purus. Sed in.\\nExperience\\n16/01 - now Developer The Company\\nProin laoreet dolor vitae velit tristique, id interdum augue finibus quisque non ades\\nErat at purus facilisis vestibulum pulvinar sit amet felis, mauris laoreet justo nec err in\\nLorem consectetur elementum, aliquam facilisis ante id magna porta mattis, vivamus\\nMorbi sit amet ullamcorper felis fusce nec mi ac nisi cursus aliquet, vestibulum volutpat\\nVestibulum quam lectus, tempus in urna semper, finibus consequat mauris, quisque\\n15/01 - 15/12 Developer The Older Company\\nPraesent aliquam sagittis hendrerit phasellus efficitur tincidunt et (https:/ /google.com)\\nAmet eget augue nam quis sapien eget arcu placerat lobortis vivamus maximus elit id\\nEget lacus nec dolor sagittis efficitur aliquam nec metus vitae justo auctor sit erat arr\\nCondimentum vestibulum, nullam vitae cursus erat, praesent hendrerit leo a turpis\\n14/01 - 14/12 Intern The Oldest Company\\nQuisque in lacus lorem. In vitae feugiat leo mauris a hendrerit felis fusce mi mattis\\nViverra ut sem non, commodo dictum leo nullam pulvinar mattis nisi quis iaculis ex\\nNulla finibus leo lectus, sed feugiat arcu ultrices non. Aenean sit amet vestibulum sit\\nLorem sed diam purus, aliquam ac diam feugiat, volutpat lobortis ex cras nec sem\\nEducation\\n2013 - 2016 Masters Degree, Computer Science The University\\nThesis: Vestibulum Vel Lorem Ex Duis Varius Lorem Iaculis Laoreet Fringilla Nam Ex\\nDignissim malesuada vestibulum sed eget elit justo aliquam eu arcu a dui interdum eget\\nDonec mattis, purus vel pellentesque maximus tellus arcu tempus elit, ut sodales felis\\nElementum mauris arcu felis, sodales et maximus in, pretium maximus urna praesent\\n2009 - 2013 Bachelors Degree, Computer Science The University\\nThesis: Duis Molestie Faucibus Ligula, Sed Suscipit Tellus Tempor Sed Maecenas Li\\nAccumsan ligula at feugiat donec gravida, odio ac sodales consectetur, quam metus\\nMolestie mi, nec cursus sem leo sit amet sapien proin feugiat efficitur suscipit duis\\nhttps:/www.linkedin.com/in/john-doe  https:/ /github.com/john-doe')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "48100c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = \"\\n\".join([doc.page_content for doc in textex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b680fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size =100, chunk_overlap = 0)\n",
    "texts=text_splitter.split_text (full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1c4ee799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.11.0-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken) (2025.9.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n",
      "Downloading tiktoken-0.11.0-cp311-cp311-win_amd64.whl (884 kB)\n",
      "   ---------------------------------------- 0.0/884.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 884.4/884.4 kB 13.2 MB/s  0:00:00\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4b9868de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John Doe  Software Developer  Prague, Czech Republic  john.doe@gmail.com  +420 123 456 789\\nJOHN DOE RESUME\\nEnnatha: Developer at The Company, M.Sc. in Computer Science\\nSkills: Java, C#/.Net, C++, Python, JavaScript, Ruby, Bash, SQL\\nInterests: Data Warehouses, Data Lakes, Data Analysis, Data Quality\\nActivities: Hockey, Football, Tennis, Basketball, Reading, Music\\nSummary\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque gravida ligula sed rhoncus lobortis. Pellen-\\ntesque sit amet sapien in neque cursus mollis. Nulla aliquet mauris ac enim fermentum tincidunt. Donec eu felis\\nut purus ullamcorper aliquet eget at lorem. Suspendisse vulputate mi libero, sit amet sollicitudin enim suscipit\\nin. Curabitur laoreet sem ultrices, consequat neque sed, vulputate purus. Sed in.\\nExperience\\n16/01 - now Developer The Company\\nProin laoreet dolor vitae velit tristique, id interdum augue finibus quisque non ades\\nErat at purus facilisis vestibulum pulvinar sit amet felis, mauris laoreet justo nec err in\\nLorem consectetur elementum, aliquam facilisis ante id magna porta mattis, vivamus\\nMorbi sit amet ullamcorper felis fusce nec mi ac nisi cursus aliquet, vestibulum volutpat\\nVestibulum quam lectus, tempus in urna semper, finibus consequat mauris, quisque\\n15/01 - 15/12 Developer The Older Company\\nPraesent aliquam sagittis hendrerit phasellus efficitur tincidunt et (https:/ /google.com)\\nAmet eget augue nam quis sapien eget arcu placerat lobortis vivamus maximus elit id\\nEget lacus nec dolor sagittis efficitur aliquam nec metus vitae justo auctor sit erat arr\\nCondimentum vestibulum, nullam vitae cursus erat, praesent hendrerit leo a turpis\\n14/01 - 14/12 Intern The Oldest Company\\nQuisque in lacus lorem. In vitae feugiat leo mauris a hendrerit felis fusce mi mattis\\nViverra ut sem non, commodo dictum leo nullam pulvinar mattis nisi quis iaculis ex\\nNulla finibus leo lectus, sed feugiat arcu ultrices non. Aenean sit amet vestibulum sit\\nLorem sed diam purus, aliquam ac diam feugiat, volutpat lobortis ex cras nec sem\\nEducation\\n2013 - 2016 Masters Degree, Computer Science The University\\nThesis: Vestibulum Vel Lorem Ex Duis Varius Lorem Iaculis Laoreet Fringilla Nam Ex\\nDignissim malesuada vestibulum sed eget elit justo aliquam eu arcu a dui interdum eget\\nDonec mattis, purus vel pellentesque maximus tellus arcu tempus elit, ut sodales felis\\nElementum mauris arcu felis, sodales et maximus in, pretium maximus urna praesent\\n2009 - 2013 Bachelors Degree, Computer Science The University\\nThesis: Duis Molestie Faucibus Ligula, Sed Suscipit Tellus Tempor Sed Maecenas Li\\nAccumsan ligula at feugiat donec gravida, odio ac sodales consectetur, quam metus\\nMolestie mi, nec cursus sem leo sit amet sapien proin feugiat efficitur suscipit duis\\nhttps:/www.linkedin.com/in/john-doe  https:/ /github.com/john-doe']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea8237c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "56744515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.3.27)\n",
      "Collecting openai\n",
      "  Downloading openai-1.107.2-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (0.3.76)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (0.4.27)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (2.11.9)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\vijay\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.10.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.11.0-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (4.56.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.7.2-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.16.2-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (0.34.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.1)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vijay\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading openai-1.107.2-py3-none-any.whl (946 kB)\n",
      "   ---------------------------------------- 0.0/946.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 946.9/946.9 kB 8.8 MB/s  0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.11.0-cp311-cp311-win_amd64.whl (204 kB)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Downloading scikit_learn-1.7.2-cp311-cp311-win_amd64.whl (8.9 MB)\n",
      "   ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 3.9/8.9 MB 19.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.6/8.9 MB 18.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.7/8.9 MB 18.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.9/8.9 MB 10.8 MB/s  0:00:00\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading scipy-1.16.2-cp311-cp311-win_amd64.whl (38.7 MB)\n",
      "   ---------------------------------------- 0.0/38.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 3.1/38.7 MB 15.4 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 7.1/38.7 MB 16.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 11.0/38.7 MB 17.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 15.2/38.7 MB 17.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 19.7/38.7 MB 18.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 24.4/38.7 MB 18.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 28.6/38.7 MB 19.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 30.7/38.7 MB 18.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 33.8/38.7 MB 17.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.3/38.7 MB 18.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.5/38.7 MB 18.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.5/38.7 MB 18.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.5/38.7 MB 18.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.7/38.7 MB 13.4 MB/s  0:00:02\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, jiter, distro, scikit-learn, openai, sentence-transformers\n",
      "\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ----- ---------------------------------- 1/8 [scipy]\n",
      "   ---------- ----------------------------- 2/8 [joblib]\n",
      "   ---------- ----------------------------- 2/8 [joblib]\n",
      "   ---------- ----------------------------- 2/8 [joblib]\n",
      "   ---------- ----------------------------- 2/8 [joblib]\n",
      "   ---------- ----------------------------- 2/8 [joblib]\n",
      "   ---------- ----------------------------- 2/8 [joblib]\n",
      "   ---------- ----------------------------- 2/8 [joblib]\n",
      "   ---------- ----------------------------- 2/8 [joblib]\n",
      "   ---------- ----------------------------- 2/8 [joblib]\n",
      "   ---------- ----------------------------- 2/8 [joblib]\n",
      "   ---------- ----------------------------- 2/8 [joblib]\n",
      "   ---------- ----------------------------- 2/8 [joblib]\n",
      "   ---------- ----------------------------- 2/8 [joblib]\n",
      "   -------------------- ------------------- 4/8 [distro]\n",
      "   -------------------- ------------------- 4/8 [distro]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------- -------------- 5/8 [scikit-learn]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ------------------------------ --------- 6/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ----------------------------------- ---- 7/8 [sentence-transformers]\n",
      "   ---------------------------------------- 8/8 [sentence-transformers]\n",
      "\n",
      "Successfully installed distro-1.9.0 jiter-0.11.0 joblib-1.5.2 openai-1.107.2 scikit-learn-1.7.2 scipy-1.16.2 sentence-transformers-5.1.0 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain openai sentence-transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5216f192",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e03246c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8eb90491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d8d7971e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vijay\\AppData\\Local\\Temp\\ipykernel_3340\\4167420869.py:1: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  openai_embed = OpenAIEmbeddings(openai_api_key=\"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\")\n"
     ]
    }
   ],
   "source": [
    "openai_embed = OpenAIEmbeddings(openai_api_key=\"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\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f37c4fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "textembed = \"Hi I am vijay. Welcome to my page\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6d1c7b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.020271869924537188, 0.002532365567698675, -0.011521049959491308]\n"
     ]
    }
   ],
   "source": [
    "embded = openai_embed.embed_query (textembed)\n",
    "print (embded[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "898e872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bd633a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "40f484e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp311-cp311-win_amd64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from faiss-cpu) (2.2.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\vijay\\appdata\\roaming\\python\\python311\\site-packages (from faiss-cpu) (25.0)\n",
      "Downloading faiss_cpu-1.12.0-cp311-cp311-win_amd64.whl (18.2 MB)\n",
      "   ---------------------------------------- 0.0/18.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.6/18.2 MB 8.4 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 3.1/18.2 MB 8.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 4.5/18.2 MB 7.3 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 5.2/18.2 MB 7.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 6.3/18.2 MB 5.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 9.4/18.2 MB 7.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 9.4/18.2 MB 7.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 11.5/18.2 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 14.4/18.2 MB 7.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 16.5/18.2 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.1/18.2 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.2/18.2 MB 7.7 MB/s  0:00:02\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf285a02",
   "metadata": {},
   "source": [
    "text =  [\n",
    "        \"Hi I am vijay. Welcome to my page\",\n",
    "        \"On the way to Number 1\",\n",
    "        \"This is a course for GenAI\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e672da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b586fccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vijay\\AppData\\Local\\Temp\\ipykernel_35228\\2961239294.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embed_new = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "c:\\Users\\Vijay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Vijay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Vijay\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "embed_new = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "111b06dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore= FAISS.from_texts(text,embed_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4def15e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x12e9fe366d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "932a8ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.save_local(\"MyFirstVectorDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96062910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chroma\n",
      "  Downloading Chroma-0.2.0.tar.gz (5.8 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: chroma\n",
      "  Building wheel for chroma (pyproject.toml): started\n",
      "  Building wheel for chroma (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for chroma: filename=chroma-0.2.0-py3-none-any.whl size=7184 sha256=74e09b9bf113408ea89ec372afc35bb34a85b3bcb5851b468e81b32ee07f8c9a\n",
      "  Stored in directory: c:\\users\\vijay\\appdata\\local\\pip\\cache\\wheels\\d9\\fb\\ff\\59d3b021a3434588b65b15f1a785b4aebca8f21ea516319194\n",
      "Successfully built chroma\n",
      "Installing collected packages: chroma\n",
      "Successfully installed chroma-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3814d95",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtext\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6ea9593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ad1946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4a0c0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vijay\\AppData\\Local\\Temp\\ipykernel_44120\\2652901427.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embdedc_new=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "c:\\Users\\Vijay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embdedc_new=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c70e453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstorec = Chroma.from_texts(text, embdedc_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c601e40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Downloading chromadb-1.0.21-cp39-abi3-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (2.11.9)\n",
      "Collecting pybase64>=1.4.1 (from chromadb)\n",
      "  Downloading pybase64-1.4.2-cp311-cp311-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (2.2.6)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (4.14.1)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.22.1-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (0.22.0)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Downloading grpcio-1.74.0-cp311-cp311-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Downloading typer-0.17.4-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.2.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (3.11.3)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Collecting rich>=10.11.0 (from chromadb)\n",
      "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (4.25.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.7 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.32.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vijay\\appdata\\roaming\\python\\python311\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\vijay\\appdata\\roaming\\python\\python311\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (2025.8.3)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\vijay\\appdata\\roaming\\python\\python311\\site-packages (from build>=1.0.3->chromadb) (25.0)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\vijay\\appdata\\roaming\\python\\python311\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: protobuf in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (6.32.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Collecting googleapis-common-protos~=1.57 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.37.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.37.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.58b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\vijay\\appdata\\roaming\\python\\python311\\site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.9.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp311-cp311-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.1.0-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-15.0.1-cp311-cp311-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Downloading chromadb-1.0.21-cp39-abi3-win_amd64.whl (19.8 MB)\n",
      "   ---------------------------------------- 0.0/19.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 2.6/19.8 MB 13.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 5.2/19.8 MB 13.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 8.1/19.8 MB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 11.3/19.8 MB 13.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 13.9/19.8 MB 13.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 17.6/19.8 MB 14.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  19.7/19.8 MB 14.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 19.8/19.8 MB 12.6 MB/s  0:00:01\n",
      "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl (152 kB)\n",
      "Downloading build-1.3.0-py3-none-any.whl (23 kB)\n",
      "Downloading grpcio-1.74.0-cp311-cp311-win_amd64.whl (4.5 MB)\n",
      "   ---------------------------------------- 0.0/4.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.5/4.5 MB 30.2 MB/s  0:00:00\n",
      "Downloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.9/1.9 MB 21.5 MB/s  0:00:00\n",
      "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading mmh3-5.2.0-cp311-cp311-win_amd64.whl (41 kB)\n",
      "Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Downloading onnxruntime-1.22.1-cp311-cp311-win_amd64.whl (12.7 MB)\n",
      "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/12.7 MB 25.4 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 1.3/12.7 MB 2.7 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 6.0/12.7 MB 9.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.3/12.7 MB 14.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.7/12.7 MB 13.3 MB/s  0:00:00\n",
      "Downloading opentelemetry_api-1.37.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl (19 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.37.0-py3-none-any.whl (72 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading opentelemetry_sdk-1.37.0-py3-none-any.whl (131 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl (207 kB)\n",
      "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading pybase64-1.4.2-cp311-cp311-win_amd64.whl (35 kB)\n",
      "Downloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading typer-0.17.4-py3-none-any.whl (46 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Downloading httptools-0.6.4-cp311-cp311-win_amd64.whl (88 kB)\n",
      "Downloading watchfiles-1.1.0-cp311-cp311-win_amd64.whl (292 kB)\n",
      "Downloading websockets-15.0.1-cp311-cp311-win_amd64.whl (176 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53916 sha256=ec62247ca9145a6a9408bf172e1a207f3227f8de682d5abf14775dbb9f3ad5df\n",
      "  Stored in directory: c:\\users\\vijay\\appdata\\local\\pip\\cache\\wheels\\a3\\01\\bd\\4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, flatbuffers, durationpy, websockets, shellingham, pyreadline3, pyproject_hooks, pybase64, pyasn1, overrides, opentelemetry-proto, oauthlib, mmh3, mdurl, importlib-resources, httptools, grpcio, googleapis-common-protos, cachetools, bcrypt, backoff, watchfiles, uvicorn, rsa, requests-oauthlib, pyasn1-modules, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, markdown-it-py, humanfriendly, build, rich, opentelemetry-semantic-conventions, google-auth, coloredlogs, typer, opentelemetry-sdk, onnxruntime, kubernetes, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
      "\n",
      "   ----------------------------------------  0/42 [pypika]\n",
      "    ---------------------------------------  1/42 [flatbuffers]\n",
      "   - --------------------------------------  2/42 [durationpy]\n",
      "   -- -------------------------------------  3/42 [websockets]\n",
      "   -- -------------------------------------  3/42 [websockets]\n",
      "   -- -------------------------------------  3/42 [websockets]\n",
      "   -- -------------------------------------  3/42 [websockets]\n",
      "   -- -------------------------------------  3/42 [websockets]\n",
      "   -- -------------------------------------  3/42 [websockets]\n",
      "   -- -------------------------------------  3/42 [websockets]\n",
      "   -- -------------------------------------  3/42 [websockets]\n",
      "   -- -------------------------------------  3/42 [websockets]\n",
      "   --- ------------------------------------  4/42 [shellingham]\n",
      "   ---- -----------------------------------  5/42 [pyreadline3]\n",
      "   ---- -----------------------------------  5/42 [pyreadline3]\n",
      "   ---- -----------------------------------  5/42 [pyreadline3]\n",
      "   ---- -----------------------------------  5/42 [pyreadline3]\n",
      "   ---- -----------------------------------  5/42 [pyreadline3]\n",
      "   ------ ---------------------------------  7/42 [pybase64]\n",
      "   ------ ---------------------------------  7/42 [pybase64]\n",
      "   ------- --------------------------------  8/42 [pyasn1]\n",
      "   ------- --------------------------------  8/42 [pyasn1]\n",
      "   ------- --------------------------------  8/42 [pyasn1]\n",
      "   ------- --------------------------------  8/42 [pyasn1]\n",
      "   -------- -------------------------------  9/42 [overrides]\n",
      "   --------- ------------------------------ 10/42 [opentelemetry-proto]\n",
      "   --------- ------------------------------ 10/42 [opentelemetry-proto]\n",
      "   --------- ------------------------------ 10/42 [opentelemetry-proto]\n",
      "   --------- ------------------------------ 10/42 [opentelemetry-proto]\n",
      "   ---------- ----------------------------- 11/42 [oauthlib]\n",
      "   ---------- ----------------------------- 11/42 [oauthlib]\n",
      "   ---------- ----------------------------- 11/42 [oauthlib]\n",
      "   ---------- ----------------------------- 11/42 [oauthlib]\n",
      "   ---------- ----------------------------- 11/42 [oauthlib]\n",
      "   ---------- ----------------------------- 11/42 [oauthlib]\n",
      "   ---------- ----------------------------- 11/42 [oauthlib]\n",
      "   ---------- ----------------------------- 11/42 [oauthlib]\n",
      "   ---------- ----------------------------- 11/42 [oauthlib]\n",
      "   ---------- ----------------------------- 11/42 [oauthlib]\n",
      "   ---------- ----------------------------- 11/42 [oauthlib]\n",
      "   ------------ --------------------------- 13/42 [mdurl]\n",
      "   ------------- -------------------------- 14/42 [importlib-resources]\n",
      "   ------------- -------------------------- 14/42 [importlib-resources]\n",
      "   ------------- -------------------------- 14/42 [importlib-resources]\n",
      "   ------------- -------------------------- 14/42 [importlib-resources]\n",
      "   --------------- ------------------------ 16/42 [grpcio]\n",
      "   --------------- ------------------------ 16/42 [grpcio]\n",
      "   --------------- ------------------------ 16/42 [grpcio]\n",
      "   --------------- ------------------------ 16/42 [grpcio]\n",
      "   --------------- ------------------------ 16/42 [grpcio]\n",
      "   --------------- ------------------------ 16/42 [grpcio]\n",
      "   --------------- ------------------------ 16/42 [grpcio]\n",
      "   --------------- ------------------------ 16/42 [grpcio]\n",
      "   ---------------- ----------------------- 17/42 [googleapis-common-protos]\n",
      "   ---------------- ----------------------- 17/42 [googleapis-common-protos]\n",
      "   ---------------- ----------------------- 17/42 [googleapis-common-protos]\n",
      "   ---------------- ----------------------- 17/42 [googleapis-common-protos]\n",
      "   ---------------- ----------------------- 17/42 [googleapis-common-protos]\n",
      "   ---------------- ----------------------- 17/42 [googleapis-common-protos]\n",
      "   ---------------- ----------------------- 17/42 [googleapis-common-protos]\n",
      "   ---------------- ----------------------- 17/42 [googleapis-common-protos]\n",
      "   ---------------- ----------------------- 17/42 [googleapis-common-protos]\n",
      "   ---------------- ----------------------- 17/42 [googleapis-common-protos]\n",
      "   ---------------- ----------------------- 17/42 [googleapis-common-protos]\n",
      "   ---------------- ----------------------- 17/42 [googleapis-common-protos]\n",
      "   ---------------- ----------------------- 17/42 [googleapis-common-protos]\n",
      "   ---------------- ----------------------- 17/42 [googleapis-common-protos]\n",
      "   ---------------- ----------------------- 17/42 [googleapis-common-protos]\n",
      "  Attempting uninstall: cachetools\n",
      "   ---------------- ----------------------- 17/42 [googleapis-common-protos]\n",
      "    Found existing installation: cachetools 6.2.0\n",
      "   ---------------- ----------------------- 17/42 [googleapis-common-protos]\n",
      "    Uninstalling cachetools-6.2.0:\n",
      "   ---------------- ----------------------- 17/42 [googleapis-common-protos]\n",
      "      Successfully uninstalled cachetools-6.2.0\n",
      "   ---------------- ----------------------- 17/42 [googleapis-common-protos]\n",
      "   ----------------- ---------------------- 18/42 [cachetools]\n",
      "   ------------------- -------------------- 20/42 [backoff]\n",
      "   -------------------- ------------------- 21/42 [watchfiles]\n",
      "   -------------------- ------------------- 21/42 [watchfiles]\n",
      "   -------------------- ------------------- 22/42 [uvicorn]\n",
      "   -------------------- ------------------- 22/42 [uvicorn]\n",
      "   -------------------- ------------------- 22/42 [uvicorn]\n",
      "   -------------------- ------------------- 22/42 [uvicorn]\n",
      "   -------------------- ------------------- 22/42 [uvicorn]\n",
      "   -------------------- ------------------- 22/42 [uvicorn]\n",
      "   --------------------- ------------------ 23/42 [rsa]\n",
      "   --------------------- ------------------ 23/42 [rsa]\n",
      "   --------------------- ------------------ 23/42 [rsa]\n",
      "   --------------------- ------------------ 23/42 [rsa]\n",
      "   --------------------- ------------------ 23/42 [rsa]\n",
      "   --------------------- ------------------ 23/42 [rsa]\n",
      "   --------------------- ------------------ 23/42 [rsa]\n",
      "   --------------------- ------------------ 23/42 [rsa]\n",
      "   --------------------- ------------------ 23/42 [rsa]\n",
      "   ---------------------- ----------------- 24/42 [requests-oauthlib]\n",
      "   ---------------------- ----------------- 24/42 [requests-oauthlib]\n",
      "   ----------------------- ---------------- 25/42 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 25/42 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 25/42 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 25/42 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 25/42 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 25/42 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 25/42 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 25/42 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 25/42 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 25/42 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 25/42 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 25/42 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 25/42 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 25/42 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 25/42 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 25/42 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 25/42 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 25/42 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 25/42 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 25/42 [pyasn1-modules]\n",
      "   ------------------------ --------------- 26/42 [posthog]\n",
      "   ------------------------ --------------- 26/42 [posthog]\n",
      "   ------------------------ --------------- 26/42 [posthog]\n",
      "   ------------------------ --------------- 26/42 [posthog]\n",
      "   ------------------------ --------------- 26/42 [posthog]\n",
      "   ------------------------ --------------- 26/42 [posthog]\n",
      "   ------------------------ --------------- 26/42 [posthog]\n",
      "   ------------------------ --------------- 26/42 [posthog]\n",
      "   ----------------- --------- 27/42 [opentelemetry-exporter-otlp-proto-common]\n",
      "   -------------------------- ------------- 28/42 [opentelemetry-api]\n",
      "   -------------------------- ------------- 28/42 [opentelemetry-api]\n",
      "   -------------------------- ------------- 28/42 [opentelemetry-api]\n",
      "   -------------------------- ------------- 28/42 [opentelemetry-api]\n",
      "   -------------------------- ------------- 28/42 [opentelemetry-api]\n",
      "   --------------------------- ------------ 29/42 [markdown-it-py]\n",
      "   --------------------------- ------------ 29/42 [markdown-it-py]\n",
      "   --------------------------- ------------ 29/42 [markdown-it-py]\n",
      "   --------------------------- ------------ 29/42 [markdown-it-py]\n",
      "   --------------------------- ------------ 29/42 [markdown-it-py]\n",
      "   --------------------------- ------------ 29/42 [markdown-it-py]\n",
      "   --------------------------- ------------ 29/42 [markdown-it-py]\n",
      "   --------------------------- ------------ 29/42 [markdown-it-py]\n",
      "   --------------------------- ------------ 29/42 [markdown-it-py]\n",
      "   --------------------------- ------------ 29/42 [markdown-it-py]\n",
      "   --------------------------- ------------ 29/42 [markdown-it-py]\n",
      "   ---------------------------- ----------- 30/42 [humanfriendly]\n",
      "   ---------------------------- ----------- 30/42 [humanfriendly]\n",
      "   ---------------------------- ----------- 30/42 [humanfriendly]\n",
      "   ---------------------------- ----------- 30/42 [humanfriendly]\n",
      "   ----------------------------- ---------- 31/42 [build]\n",
      "   ----------------------------- ---------- 31/42 [build]\n",
      "   ----------------------------- ---------- 31/42 [build]\n",
      "   ------------------------------ --------- 32/42 [rich]\n",
      "   ------------------------------ --------- 32/42 [rich]\n",
      "   ------------------------------ --------- 32/42 [rich]\n",
      "   ------------------------------ --------- 32/42 [rich]\n",
      "   ------------------------------ --------- 32/42 [rich]\n",
      "   ------------------------------ --------- 32/42 [rich]\n",
      "   ------------------------------ --------- 32/42 [rich]\n",
      "   ------------------------------ --------- 32/42 [rich]\n",
      "   ------------------------------ --------- 32/42 [rich]\n",
      "   ------------------------------ --------- 32/42 [rich]\n",
      "   ------------------------------ --------- 32/42 [rich]\n",
      "   ------------------------- ------- 33/42 [opentelemetry-semantic-conventions]\n",
      "   ------------------------- ------- 33/42 [opentelemetry-semantic-conventions]\n",
      "   ------------------------- ------- 33/42 [opentelemetry-semantic-conventions]\n",
      "   ------------------------- ------- 33/42 [opentelemetry-semantic-conventions]\n",
      "   ------------------------- ------- 33/42 [opentelemetry-semantic-conventions]\n",
      "   ------------------------- ------- 33/42 [opentelemetry-semantic-conventions]\n",
      "   ------------------------- ------- 33/42 [opentelemetry-semantic-conventions]\n",
      "   ------------------------- ------- 33/42 [opentelemetry-semantic-conventions]\n",
      "   ------------------------- ------- 33/42 [opentelemetry-semantic-conventions]\n",
      "   ------------------------- ------- 33/42 [opentelemetry-semantic-conventions]\n",
      "   ------------------------- ------- 33/42 [opentelemetry-semantic-conventions]\n",
      "   ------------------------- ------- 33/42 [opentelemetry-semantic-conventions]\n",
      "   ------------------------- ------- 33/42 [opentelemetry-semantic-conventions]\n",
      "   ------------------------- ------- 33/42 [opentelemetry-semantic-conventions]\n",
      "   ------------------------- ------- 33/42 [opentelemetry-semantic-conventions]\n",
      "   ------------------------- ------- 33/42 [opentelemetry-semantic-conventions]\n",
      "   ------------------------- ------- 33/42 [opentelemetry-semantic-conventions]\n",
      "   ------------------------- ------- 33/42 [opentelemetry-semantic-conventions]\n",
      "   -------------------------------- ------- 34/42 [google-auth]\n",
      "   -------------------------------- ------- 34/42 [google-auth]\n",
      "   -------------------------------- ------- 34/42 [google-auth]\n",
      "   -------------------------------- ------- 34/42 [google-auth]\n",
      "   -------------------------------- ------- 34/42 [google-auth]\n",
      "   -------------------------------- ------- 34/42 [google-auth]\n",
      "   -------------------------------- ------- 34/42 [google-auth]\n",
      "   -------------------------------- ------- 34/42 [google-auth]\n",
      "   -------------------------------- ------- 34/42 [google-auth]\n",
      "   -------------------------------- ------- 34/42 [google-auth]\n",
      "   -------------------------------- ------- 34/42 [google-auth]\n",
      "   -------------------------------- ------- 34/42 [google-auth]\n",
      "   --------------------------------- ------ 35/42 [coloredlogs]\n",
      "   --------------------------------- ------ 35/42 [coloredlogs]\n",
      "   ---------------------------------- ----- 36/42 [typer]\n",
      "   ---------------------------------- ----- 36/42 [typer]\n",
      "   ---------------------------------- ----- 36/42 [typer]\n",
      "   ---------------------------------- ----- 36/42 [typer]\n",
      "   ----------------------------------- ---- 37/42 [opentelemetry-sdk]\n",
      "   ----------------------------------- ---- 37/42 [opentelemetry-sdk]\n",
      "   ----------------------------------- ---- 37/42 [opentelemetry-sdk]\n",
      "   ----------------------------------- ---- 37/42 [opentelemetry-sdk]\n",
      "   ----------------------------------- ---- 37/42 [opentelemetry-sdk]\n",
      "   ----------------------------------- ---- 37/42 [opentelemetry-sdk]\n",
      "   ----------------------------------- ---- 37/42 [opentelemetry-sdk]\n",
      "   ----------------------------------- ---- 37/42 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------ --- 38/42 [onnxruntime]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ------------------------------------- -- 39/42 [kubernetes]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------  41/42 [chromadb]\n",
      "   ---------------------------------------- 42/42 [chromadb]\n",
      "\n",
      "Successfully installed backoff-2.2.1 bcrypt-4.3.0 build-1.3.0 cachetools-5.5.2 chromadb-1.0.21 coloredlogs-15.0.1 durationpy-0.10 flatbuffers-25.2.10 google-auth-2.40.3 googleapis-common-protos-1.70.0 grpcio-1.74.0 httptools-0.6.4 humanfriendly-10.0 importlib-resources-6.5.2 kubernetes-33.1.0 markdown-it-py-4.0.0 mdurl-0.1.2 mmh3-5.2.0 oauthlib-3.3.1 onnxruntime-1.22.1 opentelemetry-api-1.37.0 opentelemetry-exporter-otlp-proto-common-1.37.0 opentelemetry-exporter-otlp-proto-grpc-1.37.0 opentelemetry-proto-1.37.0 opentelemetry-sdk-1.37.0 opentelemetry-semantic-conventions-0.58b0 overrides-7.7.0 posthog-5.4.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 pybase64-1.4.2 pypika-0.48.9 pyproject_hooks-1.2.0 pyreadline3-3.5.4 requests-oauthlib-2.0.0 rich-14.1.0 rsa-4.9.1 shellingham-1.5.4 typer-0.17.4 uvicorn-0.35.0 watchfiles-1.1.0 websockets-15.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed6ac702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chroma in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "549acf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "text =  [\n",
    "        \"Hi I am vijay. Welcome to my page\",\n",
    "        \"On the way to Number 1\",\n",
    "        \"This is a course for GenAI\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4821ce8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'chromadb' from 'langchain.embeddings' (c:\\Users\\Vijay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\embeddings\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m chromadb\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'chromadb' from 'langchain.embeddings' (c:\\Users\\Vijay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\embeddings\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b35ec418",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Chroma' object has no attribute 'save_local'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mvectorstorec\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_local\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mMyFirstChromaVectorDB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Chroma' object has no attribute 'save_local'"
     ]
    }
   ],
   "source": [
    "vectorstorec.save_local(\"MyFirstChromaVectorDB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d3d7dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vijay\\AppData\\Local\\Temp\\ipykernel_44120\\2688209171.py:2: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstorec.persist()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vectorstorec = Chroma.from_texts(text, embdedc_new, persist_directory=\"MyChromadb\")\n",
    "vectorstorec.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c41a62a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstorec.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adf45761",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"GenAI?\"\n",
    "answer = retriever.get_relevant_documents(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ce270ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='This is a course for GenAI'),\n",
       " Document(metadata={}, page_content='Hi I am vijay. Welcome to my page'),\n",
       " Document(metadata={}, page_content='On the way to Number 1')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
